Neural Machine Translation
A major recent development in statistical machine translation is the adoption of neural net-
works. Neural network models promise better sharing of statistical evidence between similar
words and inclusion of rich context. This chapter introduces several neural network modeling
techniques and explains how they are applied to problems in machine translation
A Short History
Already during the last wave of neural network research in the s and s, machine trans-
lation was in the sight of researchers exploring these methods (Waibel et al., ). In fact, the
models proposed by Forcada and Ñeco () and Castaño et al. () are striking similar to
the current dominant neural machine translation approaches. However, none of these models
were trained on data sizes large enough to produce reasonable results for anything but toy ex-
amples. The computational complexity involved by far exceeded the computational resources
of that era, and hence the idea was abandoned for almost two decades.
During this hibernation period, data-driven approaches such as phrase-based statistical ma-
chine translation rose from obscurity to dominance and made machine translation a useful tool
for many applications, from information gisting to increasing the productivity of professional
translators.
The modern resurrection of neural methods in machine translation started with the integra-
tion of neural language models into traditional statistical machine translation systems. The pio-
neering work by Schwenk () showed large improvements in public evaluation campaigns.
However, these ideas were only slowly adopted, mainly due to computational concerns. The
use of GPUs for training also posed a challenge for many research groups that simply lacked
such hardware or the experience to exploit it.
Moving beyond the use in language models, neural network methods crept into other com-
ponents of traditional statistical machine translation, such as providing additional scores or ex-
tending translation tables (Schwenk, ; Lu et al., ), reordering (Kanouchi et al., ; Li
CHAPTER . NEURAL MACHINE TRANSLATION
et al., ) and pre-ordering models (de Gispert et al., ), and so on. For instance, the joint
translation and language model by Devlin et al. () was inﬂuential since it showed large
quality improvements on top of a very competitive statistical machine translation system.
More ambitious efforts aimed at pure neural machine translation, abandoning existing sta-
tistical approaches completely. Early steps were the use of convolutional models (Kalchbrenner
and Blunsom, ) and sequence-to-sequence models (Sutskever et al., ; Cho et al., ).
These were able to produce reasonable translations for short sentences, but fell apart with in-
creasing sentence length. The addition of the attention mechanism ﬁnally yielded competitive
results (Bahdanau et al., ; Jean et al., b). With a few more reﬁnements, such as byte
pair encoding and back-translation of target-side monolingual data, neural machine translation
became the new state of the art.
Within a year or two, the entire research ﬁeld of machine translation went neural. To give
some indication of the speed of change: At the shared task for machine translation organized
by the Conference on Machine Translation (WMT), only one pure neural machine translation
system was submitted in . It was competitive, but outperformed by traditional statistical
systems. A year later, in , a neural machine translation system won in almost all language
pairs. In , almost all submissions were neural machine translation systems.
At the time of writing, neural machine translation research is progressing at rapid pace.
There are many directions that are and will be explored in the coming years, ranging from
core machine learning improvements such as deeper models to more linguistically informed
models. More insight into the strength and weaknesses of neural machine translation is being
gathered and will inform future work.
There is an extensive proliferation of toolkits available for research, development, and de-
ployment of neural machine translation systems. At the time of writing, the number of toolkits
is multiplying, rather than consolidating. So, it is quite hard and premature to make speciﬁc
recommendations. Nevertheless, some of the promising toolkits are:
Nematus (based on Theano): https://github.com/EdinburghNLP/nematus
Marian (a C++ re-implementation of Nematus): https://marian-nmt.github.io/
OpenNMT (based on Torch/pyTorch): http://opennmt.net/
xnmt (based on DyNet): https://github.com/neulab/xnmt
Sockeye (based on MXNet): https://github.com/awslabs/sockeye
TT (based on Tensorﬂow): https://github.com/tensorflow/tensortensor
Introduction to Neural Networks
A neural network is a machine learning technique that takes a number of inputs and predicts
outputs. In many ways, they are not very different from other machine learning methods but
have distinct strengths.

.. INTRODUCTION TO NEURAL NETWORKS
Figure .: Graphical illustration of a linear model as a network: feature values are input nodes, arrows
are weights, and the score is an output node.
Linear Models
Linear models are a core element of statistical machine translation. A potential translation x of
a sentence is represented by a set of features hi(x). Each feature is weighted by a parameter λi
to obtain an overall score. Ignoring the exponential function that we used previously to turn
the linear model into a log-linear model, the following formula sums up the model.
λj hj(x)
score(λ, x) =
Graphically, a linear model can be illustrated by a network, where feature values are input
nodes, arrows are weights, and the score is an output node (see Figure .).
Most prominently, we use linear models to combine different components of a machine
translation system, such as the language model, the phrase translation model, the reordering
model, and properties such as the length of the sentence, or the accumulated jump distance
between phrase translations. Training methods assign a weight value λi to each such feature
hi(x), related to their importance in contributing to scoring better translations higher. In statis-
tical machine translation, this is called tuning.
However, linear models do not allow us to deﬁne more complex relationships between the
features. Let us say that we ﬁnd that for short sentences the language model is less important
than the translation model, or that average phrase translation probabilities higher than . are
similarly reasonable but any value below that is really terrible. The ﬁrst hypothetical example
implies dependence between features and the second example implies non-linear relationship
between the feature value and its impact on the ﬁnal score. Linear models cannot handle these
cases.
A commonly cited counter-example to the use of linear models is XOR, i.e., the boolean
operator ⊕ with the truth table  ⊕  = ,  ⊕  = ,  ⊕  = , and  ⊕  = . For a linear model
with two features (representing the inputs), it is not possible to come up with weights that give
the correct output in all cases. Linear models assume that all instances, represented as points
in the feature space, are linearly separable. This is not the case with XOR, and may not be the
case for type of features we use in machine translation.

CHAPTER . NEURAL MACHINE TRANSLATION
Figure .: A neural network with a hidden layer.
Multiple Layers
Neural networks modify linear models in two important ways. The ﬁrst is the use of multiple
layers. Instead of computing the output value directly from the input values, a hidden layer
is introduced. It is called hidden, because we can observe inputs and outputs in training in-
stances, but not the mechanism that connects them — this use of the concept hidden is similar
to its meaning in hidden Markov models.
See Figure . for on illustration. The network is processed in two steps. First, a linear
combination of weighted input node is computed to produce each hidden node value. Then a
linear combination of weighted hidden nodes is computed to produce each output node value.
At this point, let us introduce mathematical notations from the neural network literature.
A neural network with a hidden layer consists of
a vector of input nodes with values ⃗x = (x, x, x, ...xn)T
a vector of hidden nodes with values ⃗h = (h, h, h, ...hm)T
a vector of output nodes with values ⃗y = (y, y, y, ...yl)T
a matrix of weights connecting input nodes with hidden nodes W = {wij}
a matrix of weights connecting hidden nodes with output nodes U = {uij}
The computations in a neural network with a hidden layer, as sketched out so far, are
xiwji
hj =
hjukj
yk =
Note that we snuck in the possibility of multiple output nodes yk, although our ﬁgures so
far only showed one.

.. INTRODUCTION TO NEURAL NETWORKS
Hyperbolic tangent
Logistic function
Rectiﬁed linear unit
tanh(x) = sinh(x)
cosh(x) = ex−e−x
ex+e−x
sigmoid(x) =
+e−x
relu(x) = max(,x)
output ranges
output ranges
output ranges
from – to +
from  to +
from  to ∞
Figure .: Typical activation functions in neural networks.
Non-Linearity
If we carefully think about the addition of a hidden layer, we realize that we have not gained
anything so far to model input/output relationships. We can easily do away with the hidden
layer by multiplying out the weights
hjukj
yk =
xiwjiukj
ukjwji
xi
X
Hence, a salient element of neural networks is the use of a non-linear activation function.
After computing the linear combination of weighted feature values sj = P
i xiwji, we obtain
the value of a node only after applying such a function hj = f(sj).
Popular choices are the hyperbolic tangent tanh(x) and the logistic function sigmoid(x).
See Figure . for more details on these functions. A good way to think about these activation
functions is that they segment the range of values for the linear combination sj into
a segment where the node is turned off (values close to  for tanh, or – for sigmoid)
a transition segment where the node is partly turned on
a segment where the node is turned on (values close to )
A different popular choice is the activation function for the rectiﬁed linear unit (ReLU). It
does not allow for negative values and ﬂoors them at , but does not alter the value of positive
values. It is simpler and faster to compute than tanh(x) or sigmoid(x).
You could view each hidden node as a feature detector. For a certain conﬁgurations of input
node values, it is turned on, for others it is turned off. Advocates of neural networks claim that

CHAPTER . NEURAL MACHINE TRANSLATION
Figure .: A simple neural network with bias nodes in input and hidden layers.
the use of hidden nodes obviates (or at least drastically reduces) the need for feature engineer-
ing: Instead of manually detecting useful patterns in input values, training of the hidden nodes
discovers them automatically.
We do not have to stop at a single hidden layer. The currently fashionable name deep
learning for neural networks stems from the fact that often better performance can be achieved
by deeply stacking together layers and layers of hidden nodes.
Inference
Let us walk through neural network i¯nference (i.e., how output values are computed from input
values) with a concrete example. Consider the neural network in Figure .. This network has
one additional innovation that we have not presented so far: bias units. These are nodes that
always have the value . Such bias units give the network something to work with in the case
that all input values are . Otherwise, the weighted sum sj would be  no matter the weights.
Let us use this neural network to process some input, say the value  for the ﬁrst input
node x and  for the second input node x. The value of the bias input node (labelled x) is
ﬁxed to . To compute the value of the ﬁrst hidden node h, we have to carry out the following
calculation.
h = sigmoid
xiwi
= sigmoid ( ×  +  ×  +  × −)
= sigmoid ()
= .
The calculations for the other nodes are summarized in Table .. The output value in
node y for the input (,) is .. If we expect binary output, we would understand this result
as the value , since it is over the threshold of . in the range of possible output values [;].
Here, the output for all possible binary inputs:

.. INTRODUCTION TO NEURAL NETWORKS
Layer
Node
Summation
Activation
hidden
×  +  ×  +  × − =
hidden
×  +  ×  +  × − = −
output
. ×  + . × − +  × − = .
Table .: Calculations for input (,) to the network in Figure ..
Input x
Input x
Hidden h
Hidden h
Output y
. →
. →
. →
. →
Our neural network computes XOR. How does it do that? If we look at the hidden nodes h
and h, we notice that h acts like the Boolean OR: Its value is high if at least of the two input
values is  (h = ., ., ., for the three conﬁgurations), it otherwise has a low value
(.). The other hidden node h acts like the Boolean AND — it only has a high value (.)
if both inputs are . XOR is effectively implemented as the subtraction of the AND from the OR
hidden node.
Note that the non-linearity is key here. Since the value for the OR node h is not that much
higher for the input of (,) opposed to a single  in the input (. vs. . and .), the
distinct high value for the AND node h in this case (.) manages to push the ﬁnal output
y below the threshold. This would not be possible if the values of the inputs would be simply
summed up as in linear models.
As mentioned before, recently the use of the name deep learning for neural networks has
become fashionable. It emphasizes that often higher performance can be achieved by using
networks with multiple hidden layers. Our XOR example hints at where this power comes
from. With a single input-output layer network it is possible to mimic basic Boolean operations
such as AND and OR since they can be modeled with linear classiﬁers. XOR can be expressed as
x AND y − x OR y, and our neural network example implements the Boolean operations AND
and OR in the ﬁrst layer, and the subtraction in the second layer. For functions that require more
intricate computations, more operations may be chained together, and hence a neural network
architecture with more hidden layers may be needed. It may be possible (with sufﬁcient train-
ing data) to build neural networks for any computer program, if the number of hidden layers
matches the depth of the computation. There is a line of research under the banner neural
Turing machines that explores what kind of architectures are needed to implement basic algo-
rithms (Gemici et al., ). For instance, a neural network with two hidden layers is sufﬁcient
to implement an algorithm that sorts n-bit numbers.
Back-Propagation Training
Training neural networks requires the optimization of weight values so that the network pre-
dicts the correct output for a set of training examples. We repeatedly feed the input from the

CHAPTER . NEURAL MACHINE TRANSLATION
Current Point
Gradient for w
Optimum
Combined Gradient
Gradient for w
Figure .: Gradient descent training: We compute the gradient with regard to every dimension. In
this case the gradient with respect to weight w smaller than the gradient with respect to the weight w,
so we move more to the left than down (note: arrows point in negative gradient direction, pointing to
the minimum).
training examples into the network, compare the computed output of the network with the
correct output from the training example, and update the weights. Typically, several passes
over the training data are carried out. Each pass over the data is called an epoch.
The most common training method for neural networks is called back-propagation, since
it ﬁrst updates the weights to the output layer, and propagates back error information to earlier
layers. Whenever a training example is processed, then for each node in the network, an error
term is computed which is the basis for updating the values for incoming weights.
The formulas used to compute updated values for weights follows principles of gradient
descent training. The error for a speciﬁc node is understood as a function of the incoming
weights. To reduce the error given this function, we compute the gradient of the error function
with respect to each of the weights, and move against the gradient to reduce the error.
Why is moving alongside the gradient a good idea? Consider that we optimize multiple
dimensions at the same time. If you are looking for the lowest point in an area (maybe you are
looking for water in a desert), and the ground falls off steep to the west of you, and also slightly
south of you, then you would go in a direction that is mainly west — and only slightly south.
In other words, you go alongside the gradient. See Figure . for an illustration.
In the following two sections, we will derive the formulae for updating weights for our
example network. If you are less interested in the why and more in the how, you can skip these
sections and continue reading when we summarize the update formulae on page .
Weights to the output nodes
Let us ﬁrst review and extend our notation. At an output node yi, we ﬁrst compute a linear
combination of weight and hidden node values.
wi←jhj
si =

.. INTRODUCTION TO NEURAL NETWORKS
This sum si is passed through an activation function such as sigmoid to compute the output
value y.
yi = sigmoid(si)
(ti − yi)
We compare the computed output values yi against the target output values ti from the
training example. There are various ways to compute an error value E from these values. Let
us use the L norm.
E =
As we stated above, our goal is to compute the gradient of the error E with respect to the
weights wk to ﬁnd out in which direction (and how strongly) we should move the weight value.
We do this for each weight wk separately. We ﬁrst break up the computation of the gradient
into three steps, essentially unfolding the Equations . to ..
dE
dsi
dyi
dyi
dsi
dwi←j
= dE
dwi←j
Let us now work through each of these three steps.
Since we deﬁned the error E in terms of the output values yi, we can compute the ﬁrst
component as follows.
dE
dyi
= d
dyi
(ti − yi) = −(ti − yi)
The derivative of the output value yi with respect to si (the linear combination of weight
and hidden node values) depends on the activation function. In the case of sigmoid, we
have:
dyi
dsi
= d sigmoid(si)
dsi
= sigmoid(si)( − sigmoid(si)) = yi( − yi)
To keep our treatment below as general as possible and not commit to the sigmoid as an
activation function, we will use the shorthand y′
i for dyi
dsi below. Note that for any given
training example and any given differentiable activation function, this value can always
be computed.
Finally, we compute the derivative of si with respect to the weight wi←j, which turns out
to be quite simply the value to the hidden node hj.
ds
wi←jhj = hj
dwi←j
dwi←j
Where are we? In Equations . to ., we computed the three steps needed to compute
the gradient for the error function given the unfolded laid out in Equation .. Putting it all
together, we have
ds
dE
dyi
dyi
dsi
dwi←j
= dE
dwi←j
= −(ti − yi)
y′
hj

CHAPTER . NEURAL MACHINE TRANSLATION
Factoring in a learning rate µ gives us the following update formula for weight wi←j. Note
that we also remove the minus sign, since we move against the gradient towards the minimum.
∆wi←j = µ(ti − yi) y′
i hj
It is useful to introduce the concept of an error term δi. Note that this term is associated
with a node, while the weight updates concern weights. The error term has to be computed
only once for the node, and it can be then used for each of the incoming weights.
δi = (ti − yi) y′
This reduces the update formula to:
∆wi←j = µ δi hj
Weights to the hidden nodes
The computation of the gradient and hence the update formula for hidden nodes is quite anal-
ogous. As before, we ﬁrst deﬁne the linear combination zj (previously si) of input values xk
(previously hidden values hj) weighted by weights uj←k (previously weights wi←j).
uj←kxk
zj =
This leads to the computation of the value of the hidden node hj.
hj = sigmoid(zj)
Following the principles of gradient descent, we need to compute the derivative of the error
E with respect to the weights uj←k. We decompose this derivative as before.
dE
dzj
dhj
dhj
dzj
duj←k
= dE
duj←k
However, the computation of dE
dhj is more complex than in the case of output nodes, since
the error is deﬁned in terms of output values yi, not values for hidden nodes hj. The idea
behind back-propagation is to track how the error caused by the hidden node contributed to
the error in the next layer. Applying the chain rule gives us:
dE
dyi
dyi
dsi
dsi
dhj
dE
dhj

.. INTRODUCTION TO NEURAL NETWORKS
We already encountered the ﬁrst two terms dE
dyi (Equation .) and dyi
dsi (Equation .)
previously. To recap:
dE
dyi
dyi
dsi
= d
dyi
(ti − yi′)
y′
i′
= d
dyi
(ti − yi)
y′
= −(ti − yi)
y′
= δi
The third term in Equation . is computed straightforward.
dsi
dhj
dhj
wi←jhj = wi←j
Putting Equation . and Equation . together, Equation . can be solved as:
δiwi←j
dE
dhj
This gives rise to a quite intuitive interpretation. The error that matters at the hidden node
hj depends on the error terms δi in the subsequent nodes yi, weighted by wi←j, i.e., the impact
the hidden node hj has on the output node yi.
Let us tie up the remaining loose ends. The missing pieces from Equation . are the
second term
dhj
dzj
= d sigmoid(zj)
dzj
= sigmoid(zj)( − sigmoid(zj)) = hj( − hj) = h′
and third term
dzj
uj←kxk = xk
duj←k
duj←k
Putting Equation ., Equation ., and Equation . together gives us the gradient
dE
dzj
dhj
dhj
dzj
duj←k
= dE
duj←k
(δiwi←j) h′
j xk
If we deﬁne an error term δj for hidden nodes analogous to output nodes
(δiwi←j) h′
δj =
then we have an analogous update formula
∆uj←k = µ δj xk

CHAPTER . NEURAL MACHINE TRANSLATION
Summary
We train neural networks by processing training examples, one at a time, and update weights
each time. What drives weight updates is the gradient towards a smaller error. Weight updates
are computed based on error terms δi associated with each non-input node in the network.
For output nodes, the error term δi is computed from the actual output yi of the node for
our current network, and the target output ti for the node.
δi = (ti − yi) y′
For hidden nodes, the error term δj is computed via back-propagating the error term δi
from subsequent nodes connected by weights wi←j.
(δiwi←j) h′
δj =
Computing y′
i and h′
j requires the derivative of the activation function, to which the weighted
sum of incoming values is passed.
Given the error terms, weights wi←j (or uj←k) from each proceeding node hj (or xk) are
updated, tempered by a learning rate µ.
∆wi←j = µ δi hj
∆uj←k = µ δj xk
Once weights are updated, the next training example is processed. There are typically sev-
eral passes over the training set, called epochs.
Example
Given the neural network in Figure ., let us see how the training example (,) →  is pro-
cessed.
Let us start with the calculation of the error term δ for the output node y. During inference
(recall Table . on page ), we computed the linear combination of weighted hidden node
values s = . and the node value y = .. The target value is t = .
δ = (t − y) y′
= ( − .) × sigmoid′(.) = . × . = .
With this number, we can compute weight updates, such as for weight w←.
∆w← = µ δ h = µ × . × . = µ × .
Since the hidden node h leads only to one output node y, the calculation of its error term
δ is not more computationally complex.
(δiui←) h′
= (δ × w←) × sigmoid′(z) = . ×  × . = .
δj =
Table . summarizes the updates for all weights.

.. INTRODUCTION TO NEURAL NETWORKS
Node
Error term
Weight updates
δ = (t − y) sigmoid(s)
∆w←j = µ δ hj
δ = ( − .) × . = .
∆w← = µ × . × . = .
∆w← = µ × . × . = .
∆w← = µ × . ×  = .
δj = δ wi←j sigmoid(zj)
∆uj←i = µ δj xi
δ = . ×  × . = .
∆u← = µ × . ×  = .
∆u← = µ × . ×  =
∆u← = µ × . ×  = .
δ = . × − × . = −.
∆u← = µ × −. ×  = −.
∆u← = µ × −. ×  =
∆u← = µ × −. ×  = −.
Table .: Weight updates (with unspeciﬁed learning rate µ) for the neural network in Figure .
(repeated above the table) when the training example (,) →  is presented.
error(λ)
error(λ)
error(λ)
local optimum
global optimum
Too high learning rate
Bad initialization
Local optimum
Figure .: Problems with gradient descent training that motivate some of the reﬁnements detailed in
Section ..: (a) a too high learning rate may lead to too drastic parameter updates, overshooting the
optimum, (b) bad initialization may require many updates to escape a plateau, and (c) the existence of
local optima which trap training.

CHAPTER . NEURAL MACHINE TRANSLATION
error
validation
minimum
validation
training
training progress
Figure .: Training progress over time. The error on the training set continuously decreases. However,
on a validation set (not used for training), at some point the error increases. Training is stopped at the
validation minimum before such over-ﬁtting sets in.
Reﬁnements
We conclude our introduction to neural networks with some basic reﬁnements and consid-
erations. To motivate some of the reﬁnements, consider Figure .. While gradient descent
training is a ﬁne idea, it may run into practical problems.
Setting the learning rate too high leads to updates that overshoot the optimum. Con-
versely, a too low learning rate leads to slow convergence.
Bad initialization of weights may lead to long paths of many update steps to reach the
optimum. This is especially a problem with activation functions like sigmoid which only
have a short interval of signiﬁcant change.
The existence of local optima lead the search to get trapped and miss the global optimum.
Validation Set
Neural network training proceeds for several epochs, i.e., full iterations over the training data.
When to stop? When we track training progress, we see that the error on the training set
continuously decreases. However, at some point over-ﬁtting sets in, where the training data is
memorized and not sufﬁciently generalized.
We can check this with an additional set of examples, called the validation set, that is not
used during training. See Figure . for an illustration. When we measure the error on the
validation set at each point of training, we see that at some point this error increases. Hence,
we stop training, when the minimum on the validation set is reached.

.. INTRODUCTION TO NEURAL NETWORKS
Weight Initialization
Before training starts, weights are initialized to random values. The values are choses from a
uniform distribution. We prefer initial weights that lead to node values that are in the transition
area for the activation function, and not in the low or high shallow slope where it would take a
long time to push towards a change. For instance, for the sigmoid activation function, feeding
values in the range of, say, [−; ] to the activation function leads to activation values in the
range of [.;.].
For the sigmoid activation function, commonly used formula for weights to the ﬁnal layer
of a network are
√n,
√n
where n is the size of the previous layer. For hidden layers, we chose weights from the range
√nj + nj+
√nj + nj+
where nj is the size of the previous layer, nj size of next layer.
Momentum Term
Consider the case where a weight value is far from its optimum. Even if most training exam-
ples push the weight value in the same direction, it may still take a while for each of these
small updates to accumulate until the weight reaches its optimum. A common trick is to use
a momentum term to speed up training. This momentum term mt gets updated at each time
step t (i.e., for each training example). We combine the previous value of the momentum term
mt− with the current raw weight update value ∆wt and use the resulting momentum term
value to update the weights.
For instance, with a decay rate of ., the update formula changes to
mt = .mt− + ∆wt
wt = wt− − µ mt
Adapting Learning Rate per Parameter
A common training strategy is to reduce the learning rate µ over time. At the beginning the
parameters are far away from optimal values and have to change a lot, but in later training
stages we are concerned with ﬁne tuning, and a large learning rate may cause a parameter to
bounce around an optimum.
But different parameters may be at different stages on the path to their optimal values, so a
different learning rate for each parameter may be helpful. One such method, called Adagrad,
records the gradients that were computed for each parameter and accumulates their square
values over time, and uses this sum to adjust the learning rate.

CHAPTER . NEURAL MACHINE TRANSLATION
The Adagrad update formula is based on the sum of gradients of the error E with respect
to the weight w at all time steps t, i.e., gt = dEt
dw . We divide the learning rate µ for this weight
this accumulated sum.
∆wt =
qPt
τ= gτ
gt
Intutively, big changes in the parameter value (corresponding to big gradients gt), lead to a
reduction of the learning rate of the weight parameter.
Combining the idea of momentum term and adjusting parameter update by their accumu-
lated change is the inspiration of Adam, another method to transform the raw gradient into a
parameter update.
First, there is the idea of momentum, which is computed as in Equation . above.
mt = βmt− + ( − β)gt
Then, there is the idea of the squares of gradients (as in Adagrad) for adjusting the learning
rate. Since raw accumulation does run the risk of becoming too large and hence permanently
depressing the learning rate, Adam uses exponential decay, just like for the momentum term.
vt = βvt− + ( − β)g
The hyper parameters β and β are set typically close to , but this also means that early in
training the values for mt and vt are close to their initialization values of . To adjust for that,
they are corrected for this bias.
ˆmt =
mt
− βt
ˆvt =
vt
− βt
With increasing training time steps t, this correction goes away: limt→∞
−βt → .
Having these pieces in hand (learning rate µ, momentum ˆmt, accumulated change ˆvt),
weight update per Adam is computed as
∆wt =
√ˆvt + ϵ ˆmt
Common values for the hyper parameters are β = ., β = . and ϵ = −.
There are various other adaptation schemes. This is an active area of research. For instance,
the second order gradient gives some useful information about the rate of change. However, it
is often expensive to compute, so other shortcuts are taken.

.. INTRODUCTION TO NEURAL NETWORKS
Dropout
The parameter space in which back-propagation learning and its variants are operating is lit-
tered with local optima. The hill-climbing algorithm may just climb a mole hill and be stuck
there, instead of moving towards a climb of the highest mountain. Various methods have been
proposed to get training out of these local optima. One currently popular method in neural
machine translation is called drop-out.
It sounds a bit simplistic and wacky. During training, some of the nodes of the neural
network are ignored. Their values are set to , and their associated parameters are not updated.
These dropped-out nodes are chosen at random, and may account for as much as %, % or
even more of all the nodes. Training resumes for some number of iterations without the nodes,
and then a different set of drop-out nodes are selected.
The dropped-out nodes played some useful role in the model trained up to the point where
they are ignored. After that, other nodes have to pick up the slack. The end result is a more
robust model where several nodes share similar roles.
Layer Normalization
Layer normalization addresses a problem that arises especially in the deep neural networks
that we are using in neural machine translation, where computing proceeds through a large
sequence of layers. For some training examples, average values at one layer may become very
large, which feed into the following layer, also producing large output values, and so on. This is
especially a problem with activation functions that do not limit the output to a narrow interval,
such as rectiﬁed linear units. For other training examples the average values at the same layers
may be very small. This causes a problem for training. Recall from Equation ., that gradient
updates are strongly effected by node values. Too large node values lead to exploding gradients
and too small node values lead to diminishing gradients.
To remedy this, the idea is to normalize the values on a per-layer basis. This is done by
adding additional computational steps to the neural network. Recall that a feed-forward layer
consists of the the matrix multiplication of the weight matrix W with the node values from the
previous layer
hl−, resulting in a weighted sum ⃗sl, followed by an activation function such as
sigmoid.
⃗sl = W
hl−
⃗hl = sigmoid(⃗hl)
We can compute the mean and variance of the values in the weighted sum vector ⃗sl by
µl =
i−
sl
σl =
i−
(sl
i − µl)
CHAPTER . NEURAL MACHINE TRANSLATION
Using these values, we normalize the vector ⃗sl using two additional bias vectors ⃗g and⃗b
⃗ˆsl = ⃗g
σl  (⃗sl − µl) +⃗b
where  is element-wise multiplication and the difference subtracts the scalar average from each
vector element.
The formula ﬁrst normalizes the values in ⃗sl by shifting them against their average value,
hence ensuring that their average afterwards is . The resulting vector is then divided by
the variance σl. The additional bias vectors give some ﬂexibility, they may be shared across
multiple layers of the same type, such as multiple time steps in a recurrent neural network (we
will introduce these in Section .. on page ).
Mini Batches
Each training example yields a set of weight updates ∆wi. We may ﬁrst process all the training
examples and only afterwards apply all the updates. But neural networks have the advantage
that they can immediately learn from each training example. A training method that updates
the model with each training example is called online learning. The online learning variant of
gradient descent training is called stochastic gradient descent.
Online learning generally takes fewer passes over the training set (called epochs) for con-
vergence. However, since training constantly changes the weights, it is hard to parallelize.
So, instead, we may want to process the training data in batches, accumulate the weight up-
dates, and then apply them collectively. These smaller sets of training examples are called
mini batches to distinguish this approach from batch training where the entire training set is
considered one batch.
There are other variations to organize the processing of the training set, typically motivated
by restrictions of parallel processing. If we process the training data in mini batches, then we
can parallelize the computation of weight update values ∆w, but have to synchronize their
summation and application to the weights. If we want to distribute training over a number of
machines, it is computationally more convenient to break up the training data in equally sized
parts, perform online learning for each of the parts (optionally using smaller mini batches),
and then average the weights. Surprisingly, breaking up training this way, often leads to better
results than straightforward linear processing.
Finally, a scheme called Hogwild runs several training threads that immediately update
weights, even though other threads still use the weight values to compute gradients. While
this is clearly violates the safe guards typically taken in parallel programming, it does not hurt
in practical experience.
Vector and Matrix Operations
We can express the calculations needed for handling neural networks as vector and matrix
operations.

.. COMPUTATION GRAPHS
Forward computation: ⃗s = W ⃗h
Activation function: ⃗y = sigmoid(⃗h)
Error term: ⃗δ = (⃗t − ⃗y) sigmoid’(⃗s)
Propagation of error term: ⃗δi = W ⃗δi+  sigmoid’(⃗s)
Weight updates: ∆W = µ ⃗δ ⃗hT
Executing these operations is computationally expensive. If our layers have, say,  nodes,
then the matrix operation W⃗h requires  ×  = ,  multiplications. Such matrix oper-
ations are also common in another highly used area of computer science: graphics processing.
When rendering images on the screen, the geometric properties of -dimensional objects have
to be processed to generate the color values of the -dimensional image on the screen. Since
there is high demand for fast graphics processing, for instance for the use in realistic looking
computer games, specialized hardware has become commonplace: graphics processing units
(GPUs).
These processors have a massive number of cores (for example, the NVIDIA GTX ti
GPU provides  thread processors) but a rather lightweight instruction set. GPUs provide
instructions that are applied to many data points at once, which is exactly what is needed out
the vector space computations listed above. Programming for GPUs is supported by various
libraries, such as CUDA for C++, and has become an essential part of developing large scale
neural network applications.
The general term for scalars, vectors, and matrices is tensors. A tensor may also have more
dimensions: a sequence of matrices can be packed into a -dimensional tensor. Such large
objects are actually frequently used in today’s neural network toolkits.
Further Readings
A good introduction to modern neural network research is the textbook ”Deep
Learning” (Goodfellow et al., ). There is also book on neural network methods applied to the natural
language processing in general (Goldberg, ).
A number of key techniques that have been recently developed have entered the standard reper-
toire of neural machine translation research. Training is made more robust by methods such as drop-out
(Srivastava et al., ), where during training intervals a number of nodes are randomly masked. To
avoid exploding or vanishing gradients during back-propagation over several layers, gradients are typ-
ically clipped (Pascanu et al., ). Layer normalization (Lei Ba et al., ) has similar motivations, by
ensuring that node values are within reasonable bounds.
An active topic of research are optimization methods that adjust the learning rate of gradient descent
training. Popular methods are Adagrad (Duchi et al., ), Adadelta (Zeiler, ), and currently Adam
(Kingma and Ba, ).
Computation Graphs
For our example neural network from Section .., we painstakingly worked out derivates
for gradient computations needed by gradient descent training. After all this hard work, it may

CHAPTER . NEURAL MACHINE TRANSLATION
prod

sum

sigmoid
prod
sum
sigmoid
Figure .: Two layer feed-forward neural network as a computation graph, consisting of the input
value x, weight parameters W, W, b, b, and computation nodes (product, sum, sigmoid). To the right
of each parameter node, its value is shown. To the left of input and computation nodes, we show how
the input (, )T is processed by the graph.
come as surprise that you will likely never have to do this again. It can be done automatically,
even for arbitrarily complex neural network architectures. There are a number of toolkits that
allow you to deﬁne the network and it will take care of the rest. In this section, we will take a
close look at how this works.
Neural Networks as Computation Graphs
First, we will take a different look at the networks we are building. We previously represented
neural networks as graphs consisting of nodes and their connections (recall Figure . on
page ), or by mathematical equations such as
h = sigmoid(Wx + b)
y = sigmoid(Wh + b)
The equations above describe the feed-forward neural network that we use as our running
example. We now represent this math in form of a computation graph. See Figure . for
an illustration of the computation graph for our network. The graph contains as nodes the
parameters of the models (the weight matrices W, W and bias vectors b, b), the input x and
the mathematical operations that are carried out between them (product, sum, and sigmoid).
Next to each parameter, we show their values.

.. COMPUTATION GRAPHS
Neural networks, viewed as computation graphs, are any arbitrary connected operations
between an input and any number of parameters. Some of these operations may have little to
do with any inspiration from neurons in the brain, so we are stretching the term neural networks
quite a bit here. The graph does not have to have a nice tree structure as in our example,
but may be any acyclical directed graph, i.e., anything goes as long there is a straightforward
processing direction and no cycles. Another way to view such a graph is as a fancy way to
visualize a sequence of function calls that take as arguments the input, parameters, previously
computed values, or any combination thereof, but have no recursion or loops.
Processing an input with the neural network requires placing the input value into the node
x and carrying out the computations. In the ﬁgure, we show this with the input vector (, )T .
The resulting numbers should look familiar since they are the same as when previously worked
through this example in Section ...
Before we move on, let us take stock of what each computation node in the graph has to
accomplish. It consists of the following:
a function that executes its computation operation
links to input nodes
when processing an example, the computed value
We will add two more items to each node in the following section.
Gradient Computations
So far, we showed how the computation graph can be used process an input value. Now we
will examine how it can be used to vastly simply model training. Model training requires an
error function and the computation of gradients to derive update rules for parameters.
The ﬁrst of these is quite straightforward. To compute the error, we need to add another
computation at the end of the computation graph. This computation takes the computed out-
put value y and the given correct output value t from the training data and produces an error
value. A typical error function is the L norm
(t − y). From the view of training, the result of
the execution of the computation graph is an error value.
Now, for the more difﬁcult part — devis-
ing update rules for the parameters. Look-
ing at the computation graph, model updates
originate from the error values and propagate
back to the model parameter. Hence, we call
the computations needed to compute the up-
date values also the backward pass through
the graph, opposed to the forward pass that
computed output and error.
Calculus Refresher
In calculus, the chain rule is a formula for
computing the derivative of the composi-
tion of two or more functions. That is, if f
and g are functions, then the chain rule ex-
presses the derivative of their composition
f  g (the function which maps x to f(g(x)))
in terms of the derivatives of f and g and
the product of functions as follows:
(f ◦ g)′ = (f′ ◦ g)  g′.
This can be written more explicitly in terms
of the variable. Let F = f  g, or equiva-
lently F(x) = f(g(x)) for all x Then one

CHAPTER . NEURAL MACHINE TRANSLATION
Consider the chain of operations that con-
nect the weight matrix W to the error com-
putation.
e = L(y, t)
y = sigmoid(s)
s = sum(p, b)
p = prod(h, W)
where h are the values of the hidden layer
nodes, resulting from earlier computations.
To compute the update rule for the pa-
rameter matrix W, we view the error as a
function of these parameters and take the
derivative with respect to them, in our case
dL(W)
dW
. Recall that when we computed this
derivate we ﬁrst broke it up into steps using
the chain rule. We now do the same here.
dL(W)
dy
dsigmoid(s)
ds
dsum(p, b)
dp
dprod(h, W)
dW
= dL(sigmoid(sum(prod(h, W), b)), t)
dW
= dL(y, t)
dW
Note that for the purpose for computing an update rule for W, we treat all the other vari-
ables in this computation (the target value t, the bias vector b, the hidden node values h) as
constants. This breaks up the derivative of the error with respect to the parameters W into a
chain of derivatives along the line of the nodes of the computation graph.
Hence, all we have to do for gradient computations is to come up with derivatives for each
node in the computation graph. In our example these are
dL(y, t)
(t − y)
dy
= d
dy
= t − y
dsigmoid(s)
ds
= sigmoid(s)( − sigmoid(s))
dsum(p, b)
dp
= dp + b
dp
dprod(h, W)
dW
= dWh
dW
= h
If we want to compute the gradient update for a parameter such as W, we compute values
in a backward pass, starting from the error term y. See Figure . for an illustration.
To give more detail on the computation of the gradients in the backward pass, starting at
the bottom of the graph:

.. COMPUTATION GRAPHS
– µ

– µ
prod

sum

– µ
sigmoid
– µ
prod

sum
sigmoid
Figure .: Computation graph with gradients computed in the backward pass for the training example
(, )T → .. Gradients are computed with respect to the input of the nodes, so some nodes that have
two inputs also have two gradients. See text for details on the computations of the values.

CHAPTER . NEURAL MACHINE TRANSLATION
For the L node, we use the formula
dL(y, t)
(t − y)
dy
= d
dy
= t − y
The given target output value given as training data is t = , while we computed y =
. in the forward pass. Hence, the gradient for the L norm is  − . = .. Note
that we are using values computed in the forward pass for these gradient computations.
For the lower sigmoid node, we use the formula
dsigmoid(s)
ds
= sigmoid(s)( − sigmoid(s))
Recall that the formula for the sigmoid is sigmoid(s) =
+e−s . Plugging in the value for
s = . computed in the forward pass into this formula gives us .. The chain rule
requires us to multiply this with the value that we just computed for the L node, i.e.,
., which gives us . × . = ..
For the lower sum node, we simply copy the previous value, since the derivate is :
dsum(p, b)
dp
= dp + b
dp
Note that there are two gradients associated with the sum node. One with respect to the
output of the prod node, and one with the b parameter. In both cases, the derivative is ,
so both values are the same. Hence, the gradient in both cases is ..
For the lower prod node, we use the formula
dprod(h, W)
dW
= dWh
dW
= h
So far, we dealt with scalar values. Here we encounter vectors for the ﬁrst time: the value
of the hidden nodes h = (., .)T . The chain rule requires us to multiply this with
the previously computed scalar .:
× .
As for the sum node, there are two inputs and hence two gradients. The other gradient is
with respect to the output of the upper sigmoid node.
dprod(h, W)
dh
= dWh
dh
= W
Similarly to above, we compute
(W × .)T =
× .

.. COMPUTATION GRAPHS
Having all the gradients in place, we can now read of the relevant values for weight up-
dates. These are the gradients associated with trainable parameters. For the W weight matrix,
this is the second gradient of the prod node. So the new value for W at time step t +  is
W t+
= W t
− µdprod(x, W t
dW t
− µ
The remaining computations are carried out in very similar fashion, since they form simply
another layer of the feed-forward neural network.
Our example did not include one special case: the output of a computation may be used
multiple times in subsequent steps of a computation graphs. So, there are multiple output
nodes that feed back gradients in the back-propagation pass. In this case, we add up the gradi-
ents from these descendent steps to factor in their added impact.
Let us take a second look at what a node in a computation graph comprises:
a function that computes its value
links to input nodes (to obtain argument values)
when processing an example in the forward pass, the computed value
a function that executes its gradient computation
links to children nodes (to obtain downstream gradient values)
when processing an example in the forward pass, the computed gradient
From an object oriented programming view, a node in a computation graph provides a
forward and backward function for value and gradient computations. As instantiated in an
computation graph, it is connected with speciﬁc inputs and outputs, and is also aware of the
dimensions of its variables its value and gradient. During forward and backward pass, these
variables are ﬁlled in.
Deep Learning Frameworks
In the next sections, we will encounter various network architectures. What all of these share,
however, are the need for vector and matrix operations, as well as the computation of deriva-
tives to obtain weight update formulas. It would be quite tedious to write almost identical
code to deal with each these variants. Hence, a number of frameworks have emerged to sup-
port developing neural network methods for any chosen problem. At the time of writing, the
most prominent ones are Theano (a Python library that dymanically generates and compiles
C++ code and is build on NumPy), Torch (a machine learning library and a script language
based on the Lua programming language), pyTorch (the Python variant of Torch), DyNet (a
http://deeplearning.net/software/theano/
http://torch.ch/
http://pytorch.org/
http://dynet.readthedocs.io/

CHAPTER . NEURAL MACHINE TRANSLATION
C++ implementation by natural language processing researchers that can be used as a library
in C++ or Python), and Tensorﬂow (a more recent entry to the genre from Google).
These frameworks are less geared towards ready-to-use neural network architectures, but
provide efﬁcient implementations of the vector space operations and computation of deriva-
tives, with seamless support of GPUs. Our example from Section . can be implemented in
a few lines of Python code, as we will show in this section, using the example of Theano (other
frameworks are quite similar).
You can execute the following commands on the Python command line interface if you ﬁrst
installed Theano (pip install Theano).
> import numpy
> import theano
> import theano.tensor as T
The mapping of the input layer x to the hidden layer h uses a weight matrix W, a bias vector
b, and a mapping function which consists of the linear combination T.dot and the sigmoid
activation function.
> x = T.dmatrix()
> W = theano.shared(value=numpy.array([[.,.],[.,.]]))
> b = theano.shared(value=numpy.array([-.,-.]))
> h = T.nnet.sigmoid(T.dot(x,W)+b)
Note that we deﬁne x as a matrix. This allows us to process several training examples at
once (a sequence of vectors). A good way to think about these deﬁnitions of x and h is in term of
a functional programming language. They symbolically deﬁne operations. To actually deﬁne a
function that can be called, the Theano method function is used.
> h_function = theano.function([x], h)
> h_function([[,]])
array([[ ., .]])
This example call to h_function computes the values for the hidden nodes (compare to the
numbers in Table . on page ).
The mapping from the hidden layer h to the output layer y is deﬁned in the same fashion.
W = theano.shared(value=numpy.array([.,-.] ))
b = theano.shared(-.)
y_pred = T.nnet.sigmoid(T.dot(h,W)+b)
Again, we can deﬁne a callable function to test the full network.
> predict = theano.function([x], y_pred)
> predict([[,]])
array([[ .]])
http://www.tensorflow.org/

.. NEURAL LANGUAGE MODELS
Model training requires the deﬁnition of a cost function (we use the L norm). To formulate
it, we ﬁrst need to deﬁne the variable for the correct output. The overall cost is computed as
average over all training examples.
> y = T.dvector()
> l = (y-y_pred)**
> cost = l.mean()
Gradient descent training requires the computation of the derivative of the cost function
with respect to the model parameters (i.e., the values in the weight matrices W and W and the
bias vectors b and b. A great beneﬁt of using Theano is that it computes the derivatives for
you. The following is also an example of a function with multiple inputs and multiple outputs.
> gW, gb, gW, gb = T.grad(cost, [W,b,W,b])
We have now all we need to deﬁne training. The function updates the model parameters
and returns the current predictions and cost. It uses a learning rate of ..
> train = theano.function(inputs=[x,y],outputs=[y_pred,cost],
updates=((W, W-.*gW), (b, b-.*gb),
(W, W-.*gW), (b, b-.*gb)))
Let us deﬁne the training data.
> DATA_X = numpy.array([[,],[,],[,],[,]])
> DATA_Y = numpy.array([,,,])
> predict(DATA_X)
array([ ., . , . , .])
> train(DATA_X,DATA_Y)
[array([ ., . , . , .]),
array(.)]
The training function returns the prediction and cost before the updates. If we call the
training function again, then the predictions and cost have changed for the better.
> train(DATA_X,DATA_Y)
[array([ ., ., ., .]),
array(.)]
Typically, we would loop over the training function until convergence. As discussed above,
we may also break up the training data into mini-batches and train on one mini-batch at a time.
Neural Language Models
Neural networks are a very powerful method to model conditional probability distributions
with multiple inputs p(a|b, c, d). They are robust to unseen data points — say, an unobserved
(a,b,c,d) in the training data. Using traditional statistical estimation methods, we may address
such a sparse data problem with back-off and clustering, which require insight into the prob-
lem (what part of the conditioning context to drop ﬁrst?) and arbitrary choices (how many
clusters?).

CHAPTER . NEURAL MACHINE TRANSLATION
Word
Word
Word
Word
Hidden Layer
Word
Figure .: Sketch of a neural language model: We predict a word wi based on its preceding words.
N-gram language models which reduce the probability of a sentence to the product of word
probabilities in the context of a few previous words — say, p(wi|wi−, wi−, wi−, wi−). Such
models are a prime example for a conditional probability distribution with a rich conditioning
context for which we often lack data points and would like to cluster information. In statistical
language models, complex discounting and back-off schemes are used to balance rich evidence
from lower order models — say, the bigram model p(wi|wi−) — with the sparse estimates from
high order models. Now, we turn to neural networks for help.
Feed-Forward Neural Language Models
Figure . gives a basic sketch of a -gram neural network language model. Network nodes
representing the context words have connections to a hidden layer, which connects to the out-
put layer for the predicted word.
Representing Words
We are immediately faced with a difﬁcult question: How do we rep-
resent words? Nodes in a neural network carry real-numbered values, but words are discrete
items out of a very large vocabulary. We cannot simply use token IDs, since the neural network
will assume that token , is very similar to token , — while in practice these num-
bers are completely arbitrary. The same arguments applies to the idea of using bit encoding
for token IDs. The words (, , , , , , , )T and (, , , , , , , )T have very similar encod-
ings but may have nothing to do with each other. While the idea of using such bit vectors is
occasionally explored, it does not appear to have any beneﬁts over what we consider next.
Instead, we will represent each word with a high-dimensional vector, one dimension per
word in the vocabulary, and the value  for the dimension that matches the word, and  for the
rest. The type of vectors are called one hot vector. For instance:
dog = (, , , , , , , , , ...)T
cat = (, , , , , , , , , ...)T
eat = (, , , , , , , , , ...)T

.. NEURAL LANGUAGE MODELS
Word
Word
Word
Word
Hidden Layer
Word
Figure .: Full architecture of a feed-forward neural network language model.
Context words
(wi−, wi−, wi−, wi−) are represented in a one-hot vector, then projected into continuous space as
word embeddings (using the same weight matrix C for all words). The predicted word is computed
as a one-hot vector via a hidden layer.
These are very large vectors, and we will continue to wrestle with the impact of this choice
to represent words. One stopgap is to limit the vocabulary to the most frequent, say, ,
words, and pool all the other words in an OTHER token. We could also use word classes (either
automatic clusters or linguistically motivated classes such as part-of-speech tags) to reduce the
dimensionality of the vectors. We will revisit the problem of large vocabularies later.
To pool evidence between words, we introduce another layer between the input layer and
the hidden layer. In this layer, each context word is individually projected into a lower dimen-
sional space. We use the same weight matrix for each of the context words, thus generating a
continuous space representation for each word, independent of its position in the conditioning
context. This representation is commonly referred to as word embedding.
Words that occur in similar contexts should have similar word embeddings. For instance,
if the training data for a language model frequently contains the n-grams
but the cute dog jumped
but the cute cat jumped
child hugged the cat tightly
child hugged the dog tightly
like to watch cat videos
like to watch dog videos
then the language model would beneﬁt from the knowledge that dog and cat occur in similar
contexts and hence are somewhat interchangeable. If we like to predict from a context where
dog occurs but we have seen this context only with the word cat, then we would still like to treat
this as positive evidence. Word embeddings enable generalizing between words (clustering)
and hence having robust predictions in unseen contexts (back-off).

CHAPTER . NEURAL MACHINE TRANSLATION
Neural Network Architecture
See Figure . for a visualization of the architecture the fully
ﬂedged feed forward neural network language model, consisting of the context words as one-
hot-vector input layer, the word embedding layer, the hidden layer and predicted output word
layer.
The context words are ﬁrst encoded as one-hot vectors. These are then passed through the
embedding matrix C, resulting in a vector of ﬂoating point numbers, the word embedding.
This embedding vector has typically in the order of  or  nodes. Note that we use the
same embedding matrix C for all the context words.
Also note that mathematically there is not all that much going on here. Since the input to
the multiplication to the matrix C is a one hot vector, most of the input values to the matrix
multiplication are zeros. So, practically, we are selecting the one column in the matrix that
corresponds to the input word ID. Hence, there is no use for an activation function here. In a
way, the embedding matrix a lookup table C(wj) for word embeddings, indexed by the word
ID wj.
C(wj) = C wj
Mapping to the hidden layer in the model requires concatenation of all context word em-
beddings C(wj) as input to a typical feed-forward layer, say, using tanh as activation function.
HjC(wj)

h = tanh

bh +
The output layer is interpreted as a probability distribution over words. As before, ﬁrst the
linear combination si of weights wij and hidden node values hj is computed for each node i.
s = W h
To ensure that it is indeed a proper probability distribution, we use the softmax activation
function to ensure that all values add up to one.
pi = softmax(si,⃗s) =
esi
j esj
What we described here is close to the neural probabilistic language model proposed by
Bengio et al. (). This model had one more twist, it added direct connections of the context
word embeddings to the output word. So, Equation . is replaced by
U C(wj)
s = W h +
Their paper reports that having such direct connections from context words to output
words speeds up training, although does not ultimately improve performance. We will en-
counter the idea of short-cutting hidden layers again a bit later when we discuss deeper models
with more hidden layers. They are also called residual connections, skip connections, or even
highway connections.

.. NEURAL LANGUAGE MODELS
Training
We train the parameters of a neural language model (word embedding matrix, weight
matrices, bias vectors) by processing all the n-grams in the training corpus. For each n-gram,
we feed the context words into the network and match the network’s output against the one-
hot vector of the correct word to be predicted. Weights are updated using back-propagation
(we will go into details in the next section).
Language models are commonly evaluated by perplexity, which is related to the probability
given to proper English text. A language model that likes proper English is a good language
model. Hence, the training objective for language models is to increase the likelihood of the
training data.
During training, given a context x = (wn−, wn−, wn−, wn−), we have the correct value
for the -hot vector ⃗y. For each training example (x, ⃗y), likelihood is deﬁned as
yk log pk
L(x, ⃗y; W) = −
Note that only one value yk is , the others are . So this really comes down to the probability
pk given to the correct word k. Deﬁning likelihood this way allows us to update all weights,
also the one that lead to the wrong output words.
Word Embedding
Before we move on, it is worth reﬂecting the role of word embeddings in neural machine trans-
lation and many other natural language processing tasks. We introduced them here as compact
encoding of words in relatively high-dimensional space, say  or  ﬂoating point num-
bers. In the ﬁeld of natural language processing, at the time of this writing, word embeddings
have acquired the reputation of almost magical quality.
Consider the role they play in the neural language language that we just described. They
represent context words to enable prediction the next word in a sequence.
Recall part of our earlier example:
but the cute dog jumped
but the cute cat jumped
Since dog and cat occur in similar contexts, their inﬂuence on predicting the word jumped
should be similar. It should be different from words such as dress which is unlikely to trigger
the completion jumped. The idea that words that occur in similar contexts are semantically
similar is a powerful idea in lexical semantics.
At this point in the argument, researchers love to cite John Rupert Firth:
You shall know a word by the company it keeps.
Or, as Ludwig Wittgenstein put it a bit more broadly:

CHAPTER . NEURAL MACHINE TRANSLATION
Figure .: Word embeddings projected into D. Semantically similar words occur close to each other.
The meaning of a word is its use.
Meaning and semantics are quite difﬁcult concepts with largely unresolved deﬁnition. The
idea of distributional lexical semantics is to deﬁne word meaning by their distributional prop-
erties, i.e., in which contexts they occur. Words that occur in similar contexts (dog and cat)
should have similar representations. In vector space models, such as the word embeddings
that we use here, similarity can be measured by a distance function, e.g., the cosine distance —
the angle between the vectors.
If we project the high-dimensional word embeddings down to two dimensions, we can
visualize word embeddings as shown in Figure .. In this ﬁgure, words that are similar
(drama, theater, festival) are clustered together.
But why stop there? We would like to have semantic representations so we can carry out
semantic inference such as
queen = king + (woman – man)
queens = queen + (kings – king)
Indeed there is some evidence that word embedding allow just that (Mikolov et al., ).
However, we better stop here and just note that word embeddings are a crucial tool in neural
machine translation.

.. NEURAL LANGUAGE MODELS
Efﬁcient Inference and Training
Training a neural language model is computationally expensive. For billion word corpora, even
with the use of GPUs, training takes several days with modern compute clusters. Even using
a neural language model as a scoring component in statistical machine translation decoding
requires a lot of computation. We could restrict its use only to re-ranking n-best lists or lattices,
or consider more efﬁcient methods for inference and training.
Caching for Inference
However, with a few considerations, it is actually possible to use this
neural language model within the decoder.
Word embeddings are ﬁxed for the words, so do not actually need to carry out the map-
ping from one-hot vectors to word embeddings, but just store them beforehand.
The computation between embeddings and the hidden layer can be also partly carried
out ofﬂine. Note that each word can occur in one of the  slots for conditioning context
(assuming a -gram language model). For each of the slots, we can pre-compute the
matrix multiplication of word embedding vector and the corresponding submatrix of
weights. So, at run time, we only have to sum up these pre-computations at the hidden
layer and apply the activation function.
Computing the value for each output node is insanely expensive, since there are as many
output nodes as vocabulary items. However, we are interested only in the score for a
given word that was produced by the translation model. If we only compute its node
value, we have a score that we can use.
The last point requires a longer discussion. If we compute the node value only for the word
that we want to score with the language model, we are missing an important step. To obtain a
proper probability, we need to normalize it, which requires the computation of the values for
all the other nodes.
We could simply ignore this problem and use the scores at face value. More likely words
given a context will get higher scores than less likely words, and that is the main objective. But
since we place no constraints on the scores, we may work with models where some contexts
give high scores to many words, while some contexts do not give preference for any.
It would be great, if the node values in the ﬁnal layer were already normalized probabilities.
There are methods to enforce this during training. Let us ﬁrst discuss training in detail, and
then move to these methods in Section ...
Noise Contrastive Estimation
We discussed earlier the problem that computing probabili-
ties with a neural language model is very expensive due to the need to normalize the output
node values yi using the softmax function. This requires computing values for all output nodes,
even if we are only interested in the score for a particular n-gram. To overcome the need for

CHAPTER . NEURAL MACHINE TRANSLATION
this explicit normalization step, we would like to train a model that already has yi values that
are normalized.
One way is to include the constraint that the normalization factor Z(x) = P
j esj is close
to  in the objective function. So, instead of the just the simple likelihood objective, we may
include the L norm of the log of this factor. Note that if log Z(x) ≃ , then Z(x) ≃ .
yk log pk − α log Z(x)
L(x, ⃗y; W) = −
Another way to train a self-normalizing model is called noise contrastive estimation. The
main idea is to optimize the model so that it can separate correct training examples from arti-
ﬁcially created noise examples. This method needs less computation during training, since it
does not require the computation of all output node values.
Formally, we are trying to learn the model distribution pm(⃗y|x; W). Given a noise distribu-
tion pn(⃗y|x) — in our case of language modeling a unigram model pn(⃗y) is a good choice — we
ﬁrst generate a set of noise examples Un in addition to the correct training examples Ut. If both
sets have the same size |Un| = |Ut|, then the probability that a given example (x; ⃗y) ∈ Un ∪ Ut is
predicted to be a correct training example is
p(correct|x, ⃗y) =
pm(⃗y|x; W)
pm(⃗y|x; W) + pn(⃗y|x)
The objective of noise contrastive estimation is to maximize p(correct|x, ⃗y) for correct train-
ing examples (x; ⃗y) ∈ Ut and to minimize it for noise examples (x; ⃗y) ∈ Un.
Using log-
likelihood, we deﬁne the objective function as
L =
|Ut|
|Un|
(x;⃗y)∈Ut
log p(correct|x, ⃗y) +
(x;⃗y)∈Un
log ( − p(correct|x, ⃗y))
Returning the the original goal of a self-normalizing model, ﬁrst note that the noise distri-
bution pn(⃗y|x) is normalized. Hence, the model distribution is encouraged to produce compa-
rable values. If pm(⃗y|x; W) would generally overshoot — i.e., P
⃗y pm(⃗y|x; W) >  then it would
also give too high values for noise examples. Conversely, generally undershooting would give
too low values to correct translation examples.
Training is faster, since we only need to compute the output node value for the given train-
ing and noise examples — there is no need to compute the other values, since we do not nor-
malize with the softmax function.
Given the deﬁnition of the training objective L, we have now a complete computation graph
that we can implement using standard deep learning toolkits, as we have done before. These
toolkits compute the gradients dL
dW for all parameters W and use them for parameter updates
via gradient descent training (or its variants).
It may not be immediately obvious why optimizing towards classifying correct against
noise examples gives rise to a model that also predicts the correct probabilities for n-grams.
But this is a variant of methods that are common in statistical machine translation in the tuning
phase. MIRA (margin infused relaxation algorithm) and PRO (pairwise ranked optimization)
follow the same principle.

.. NEURAL LANGUAGE MODELS
Word
Word
copy values
Word
Word
copy values
Word
Word
Figure .: Recurrent neural language models: After predicting Word  in the context of following
Word , we re-use this hidden layer (alongside the correct Word ) to predict Word . Again, the hidden
layer of this prediction is re-used for the prediction of Word .
Recurrent Neural Language Models
The feed-forward neural language model that we described above is able to use longer con-
texts than traditional statistical back-off models, since it has more ﬂexible means to deal with
unknown contexts. Namely, the use of word embeddings to make use of similar words, and
the robust handling of unseen words in any context position. Hence, it is possible to condition
on much larger contexts than traditional statistical models. In fact, large models, say, -gram
models, have been reported to be used.
Alternatively, instead of using a ﬁxed context word window, recurrent neural networks
may condition on context sequences of any length. The trick is to re-use the hidden layer when
predicting word wn as additional input to predict word wn−.
See Figure . for an illustration. Initially, the model does not look any different from the
feed-forward neural language model that we discussed so far. The inputs to the network is the
ﬁrst word of the sentence w and a second set of neurons which at this point indicate the start
of the sentence. The word embedding of w and the start-of-sentence neurons ﬁrst map into a
hidden layer h, which is then used to predict the output word w.
This model uses the same architecture as before: Words (input and output) are represented
with one-hot vectors; word embeddings and the hidden layer use, say,  real valued neurons.
We use a sigmoid activation function at the hidden layer and the softmax function at the output
layer.
Things get interesting when we move to predicting the third word w in the sequence. One
input is the directly preceding (and now known) word w, as before. However, the neurons
in the network that we used to represent start-of-sentence are now ﬁlled with values from
the hidden layer of the previous prediction of word w. In a way, these neurons encode the

CHAPTER . NEURAL MACHINE TRANSLATION
Word
Word
Word
Word
Word
Word
Figure .: Back-propagation through time: By unfolding the recurrent neural network over a ﬁxed
number of prediction steps (here: ), we can derive update formulas based on the training objective of
predicting all output words and back-propagation of the error via gradient descent.
previous sentence context. They are enriched at each step with information about a new input
word and are hence conditioned on the full history of the sentence. So, even the last word of
the sentence is conditioned in part on the ﬁrst word of the sentence. Moreover, the model is
simpler: it has less weights than a -gram feed-forward neural language model.
How do we train such a model with arbitrarily long contexts?
One idea: At the initial stage (predicting the second word from the ﬁrst), we have the same
architecture and hence the same training procedure as for feed-forward neural networks. We
assess the error at the output layer and propagate updates back to the input layer. We could
process every training example this way — essentially by treating the hidden layer from the
previous training example as ﬁxed input the current example. However, this way, we never
provide feedback to the representation of prior history in the hidden layer.
The back-propagation through time training procedure (see Figure .) unfolds the re-
current neural network over a ﬁxed number of steps, by going back over, say,  word predic-
tions. Note that, despite limiting the unfolding to  time steps, the network is still able to learn
dependencies over longer distances.
Back-propagation through time can be either applied for each training example (here called
time step), but this is computationally quite expensive. Each time computations have to be
carried out over several steps. Instead, we can compute and apply weight updates in mini-
batches (recall Section ..). First, we process a larger number of training examples (say,
-, or the entire sentence), and then update the weights.
Given modern compute power, fully unfolding the recurrent neural network has become
more common. While recurrent neural networks have in theory arbitrary length, given a spe-
ciﬁc training example, its size is actually known and ﬁxed, so we can fully construct the com-
putation graph for each given training example, deﬁne the error as the sum of word prediction
errors, and then carry out back-propagation over the entire sentence. This does require that we
can quickly build computation graphs — so-called dynamic computation graphs — which is
currently supported by some toolkits better than others.

.. NEURAL LANGUAGE MODELS
Long Short-Term Memory Models
Consider the following step during word prediction in a sequential language model:
After much economic progress over the years, the country → has
The directly preceding word country will be the most informative for the prediction of the
word has, all the previous words are much less relevant. In general, the importance of words
decays with distance. The hidden state in the recurrent neural network will always be updated
with the most recent word, and its memory of older words is likely to diminish over time.
But sometimes, more distant words are much more important, as the following example
shows:
The country which has made much economic progress over the years still → has
In this example, the inﬂection of the verb have depends on the subject country which is
separated by a long subordinate clause.
Recurrent neural networks allow modeling of arbitrarily long sequences. Their architecture
is very simple. But this simplicity causes a number of problems.
The hidden layer plays double duty as memory of the network and as continuous space
representation used to predict output words.
While we may sometimes want to pay more attention to the directly previous word, and
sometimes pay more attention to the longer context, there is no clear mechanism to con-
trol that.
If we train the model on long sequences, then any update needs to back propagate to the
beginning of the sentence. However, propagating through so many steps raises concerns
that the impact of recent information at any step drowns out older information.
The rather confusingly named long short-term memory (LSTM) neural network architec-
ture addresses these issues. Its design is quite elaborate, although it is not very difﬁcult to use
in practice.
A core distinction is that the basic building block of LSTM networks, the so-called cell,
contains an explicit memory state. The memory state in the cell is motivated by digital memory
cells in ordinary computers. Digital memory cells offer operations to read, write, and reset.
While a digital memory cell may store just a single bit, a LSTM cell stores a real number.
Furthermore, the read/write/reset operations in a LSTM cell are regulated with a real num-
bered parameter, which are called gates (see Figure .).
Note that there is a corresponding exploding gradient problem, where over long distance gradient values
become too large. This is typically suppressed by clipping gradients, i.e., limiting them to a maximum value set as
a hyper parameter.

CHAPTER . NEURAL MACHINE TRANSLATION
LSTM Layer Time t-
forget gate
Preceding Layer
⊗ ⊕
Next Layer
output gate
input gate
LSTM Layer Time t
Figure .: A cell in a LSTM neural network. As recurrent neural networks, it receives input from the
preceeding layer (x) and the hidden layer values from the previous time step t − . The memory state m
is updated from the input state i and the previous time’s value of the memory state mt−. Various gates
channel information ﬂow in the cell towards the output value o.
The input gate parameter regulates how much new input changes the memory state.
The forget gate parameter regulates how much of the prior memory state is retained (or
forgotten).
The output gate parameter regulates how strongly the memory state is passed on to the
next layer.
Formally, marking the input, memory, and output values with the time step t, we deﬁne the
ﬂow of information within a cell as follows.
memoryt = gateinput × inputt + gateforget × memoryt−
outputt = gateoutput × memoryt
The hidden node value ht passed on to the next layer is the application of an activation
function f to the output value.
ht = f(outputt)
An LSTM layer consists of a vector of LSTM cells, just as traditional layers consist of a
vector of nodes. The input to LSTM layer is computed in the same way as the input to a
recurrent neural network node. Given the node values for the prior layer xt and the values for
the hidden layer from the previous time step ht−, the input value is the typical combination of
matrix multiplication with weights W x and W h and an activation function g.
inputt = g

W xxt + W hht−

.. NEURAL LANGUAGE MODELS
But how are the gate parameters set? They actually play a fairly important role. In par-
ticular contexts, we would like to give preference to recent input (gateinput ≃ ), rather re-
tain past memory (gateforget ≃ ), or pay less attention to the cell at the current point in time
(gateoutput ≃ ). Hence, this decision has to be informed by a broad view of the context.
How do we compute a value from such a complex conditioning context? Well, we treat it
like a node in a neural network. For each gate a ∈ (input, forget, output) we deﬁne matrices
W xa, W ha, and W ma to compute the gate parameter value by the multiplication of weights and
node values in the previous layer xt, the hidden layer ht− at the previous time step, and the
memory states at the previous time step memoryt−, followed by an activation function h.
gatea = h

W xaxt + W haht− + W mamemoryt−
LSTM are trained the same way as recurrent neural networks, using back-propagation
through time or fully unrolling the network. While the operations within a LSTM cell are more
complex than in a recurrent neural network, all the operations are still based on matrix mul-
tiplications and differentiable activation functions. Hence, we can compute gradients for the
objective function with respect to all parameters of the model and compute update functions.
Gated Recurrent Units
LSTM cells add a large number of additional parameters. For each gate alone, multiple weight
matrices are added. More parameters lead to longer training times and risk overﬁtting. As
a simpler alternative, gated recurrent units (GRU) have been proposed and used in neural
translation models. At the time of writing, LSTM cells seem to make a comeback in neural
machine translation, but both are still commonly used.
See Figure . for an illustration for GRU cells. There is no separate memory state, just
a hidden state that serves both purposes. Also, there are only two gates. These gates are
predicted as before from the input and the previous state.
updatet = g(Wupdate inputt + Uupdate statet− + biasupdate)
resett = g(Wreset
inputt + Ureset
statet− + biasreset)
The ﬁrst gate is used in the combination of the input and previous state. This is combination
is identical to traditional recurrent neural network, except that the previous states impact is
scaled by the reset gate. Since the gate’s value is between  and , this may give preference to
the current input.
combinationt = f(W inputt + U(resett ◦ statet−))
Then, the update gate is used for a interpolation of the previous state and the just computed
combination. This is done as a weighted sum, where the update gate balances between the two.

CHAPTER . NEURAL MACHINE TRANSLATION
GRU Layer Time t-
reset gate
Next Layer
update gate
Preceding Layer
GRU Layer Time t
Figure .: Gated Recurrent Unit (GRU): a simpliﬁcation of long short term memory (LSTM) cells.
statet =( − updatet) ◦ statet− +
updatet
◦ combinationt) + bias
In one extreme case, the update gate is , and the previous state is passed through directly.
In another extreme case, the update gate is , and the new state is mainly determined from the
input, with as much impact from the previous state as the reset gate allows.
It may seem a bit redundant to have two operations with a gate each that combine prior
state and input. However, these play different roles. The ﬁrst operation yielding combinationt
(Equation .) is a classic recurrent neural network component that allows more complex
computations in the combination of input and output.
The second operation yielding the
new hidden state and the output of the unit (Equation .) allows for bypassing of the in-
put, enabling long-distant memory that simply passes through information and, during back-
propagation, passes through the gradient, thus enabling long-distance dependencies.
Deep Models
The currently fashionable name deep learning for the latest wave of neural network research
has a real motivation. Large gains have been seen in tasks such as vision and speech recognition
due to stacking multiple hidden layers together.
More layers allow for more complex computations, just as having sequences of traditional
computation components (Boolean gates) allows for more complex computations such as addi-
tion and multiplication of numbers. While this has been generally recognized for a long time,
modern hardware ﬁnally enabled to train such deep neural networks on real world problems.

.. NEURAL LANGUAGE MODELS
Input
Input
Hidden
Layer
Hidden
Layer
Input
Hidden
Layer
Hidden
Layer
Hidden
Layer
Hidden
Layer
Hidden
Layer
Output
Output
Output
Shallow
Deep Stacked
Deep Transition
Figure .: Deep recurrent neural networks. The input is passed through a few hidden layers before
an output prediction is made. In deep stacked models, the hidden layers are also connected horizontally,
i.e., a layer’s values at time step t depends on its value at time step t −  as well as the previous layer
at time step t. In deep transitional models, the layers at any time step t are sequentially connected and
ﬁrst hidden layer is also informed by the last layer at time step t − .
And we learned from experiments in vision and speech that having a handful, and even dozens
of layers does give increasingly better quality.
How does the idea of deep neural networks apply to the sequence prediction tasks common
in language? There are several options. Figure . gives two examples. In shallow neural
networks, the input is passed to a single hidden layer, from which the output is predicted.
Now, a sequence of hidden layers is used. These hidden layers ht,i may be deeply stacked, so
that each layer acts like the hidden layer in the shallow recurrent neural network. Its state is
conditioned on its value at the previous time step ht−,i and the value of previous layer in the
sequence ht,i−.
ht, = f(ht−,, xt)
ﬁrst layer
ht,i = fi(ht−,i, ht,i−)
for i >
yt = fi+(ht,I)
prediction from last layer I
Or, the hidden layers may be directly connected in deep transitional networks, where the
ﬁrst hidden layer ht, is informed by the last hidden layer at the previous time step ht−,I, but
all other hidden layers are not connected to values from previous time steps.
ht, = f(ht−,I, xt)
ﬁrst layer
ht,i = fi(ht,i−)
for i >
yt = fi+(ht,I)
prediction from last layer I

CHAPTER . NEURAL MACHINE TRANSLATION
In all these equations, the function fi may be a feedforward layer (matrix multiplication
plus activation function), an LSTM cell or a GRU cell.
Experiments with using neural language models in traditional statistical machine transla-
tion have shown beneﬁts with – hidden layers (Luong et al., a).
While modern hardware allows training of deep models, they do stretch computational
resources to their practical limit. Not only are there more computations in the neural network,
convergence of training is typically slower. Adding skip connections (linking the input directly
to the output or the ﬁnal hidden layer) sometimes speeds up training, but we still talking about
a several times longer training times than shallow networks.
Further Readings
The ﬁrst vanguard of neural network research tackled language models.
prominent reference for neural language model is Bengio et al. (), who implement an n-gram lan-
guage model as a feed-forward neural network with the history words as input and the predicted word
as output. Schwenk et al. () introduce such language models to machine translation (also called
“continuous space language models"), and use them in re-ranking, similar to the earlier work in speech
recognition. Schwenk () propose a number of speed-ups. They made their implementation avail-
able as a open source toolkit (Schwenk, ), which also supports training on a graphical processing
unit (GPU) (Schwenk et al., ).
By ﬁrst clustering words into classes and encoding words as pair of class and word-in-class bits,
Baltescu et al. () reduce the computational complexity sufﬁciently to allow integration of the neural
network language model into the decoder. Another way to reduce computational complexity to enable
decoder integration is the use of noise contrastive estimation by Vaswani et al. (), which roughly
self-normalizes the output scores of the model during training, hence removing the need to compute
the values for all possible output words. Baltescu and Blunsom () compare the two techniques -
class-based word encoding with normalized scores vs. noise-contrastive estimation without normalized
scores - and show that the letter gives better performance with much higher speed.
As another way to allow straightforward decoder integration, Wang et al. () convert a contin-
uous space language model for a short list of  words into a traditional n-gram language model in
ARPA (SRILM) format. Wang et al. () present a method to merge (or “grow") a continuous space
language model with a traditional n-gram language model, to take advantage of both better estimate for
the words in the short list and the full coverage from the traditional model.
Finch et al. () use a recurrent neural network language model to rescore n-best lists for a translit-
eration system. Sundermeyer et al. () compare feed-forward with long short-term neural network
language models, a variant of recurrent neural network language models, showing better performance
for the latter in a speech recognition re-ranking task. Mikolov () reports signiﬁcant improvements
with reranking n-best lists of machine translation systems with a recurrent neural network language
model.
Neural language model are not deep learning models in the sense that they use a lot of hidden
layers. Luong et al. (a) show that having - hidden layers improves over having just the typical
layer.
Language Models in Neural Machine Translation: Traditional statistical machine translation models
have a straightforward mechanism to integrate additional knowledge sources, such as a large out of
domain language model. It is harder for end-to-end neural machine translation. Gülçehre et al. ()
add a language model trained on additional monolingual data to this model, in form of a recurrently

.. NEURAL TRANSLATION MODELS
<s>
</s>
the
house
is
big
das
Haus
ist
groß
Given
word
Embedding
Hidden
state
Predicted
word
the
house
is
big
</s>
das
Haus
ist
groß
</s>
Figure .: Sequence-to-sequence encoder-decoder model: Extending the language model, we con-
catenate the English input sentence the house is big with the German output sentence das Haus ist groß.
The ﬁrst dark green box (after processing the end-of-sentence token </s>) contains the embedding of
the entire input sentence .
neural network that runs in parallel. They compare the use of the language model in re-ranking (or, re-
scoring) against deeper integration where a gated unit regulates the relative contribution of the language
model and the translation model when predicting a word.
Neural Translation Models
We are ﬁnally prepared to look at actual translation models. We have already done most of
the work, however, since the most commonly used architecture for neural machine translation
is a straightforward extension of neural language models with one reﬁnement, an alignment
model.
Encoder-Decoder Approach
Our ﬁrst stab at a neural translation model is a straightforward extension of the language
model. Recall the idea of a recurrent neural network to model language as a sequential process.
Given all previous words, such a model predicts the next word. When we reach the end of the
sentence, we now proceed to predict the translation of the sentence, one word at a time.
See Figure . for an illustration. To train such a model, we simply concatenate the input
and output sentences and use the same method as to train a language model. For decoding, we
feed in the input sentence, and then go through the predictions of the model until it predicts an
end of sentence token.
How does such a network work? Once processing reaches the end of the input sentence
(having predicted the end of sentence marker </s>), the hidden state encodes its meaning. In
other words, the vector holding the values of the nodes of this ﬁnal hidden layer is the input
sentence embedding. This is the encoder phase of the model. Then this hidden state is used
to produce the translation in the decoder phase.

CHAPTER . NEURAL MACHINE TRANSLATION
Clearly, we are asking a lot from the hidden state in the recurrent neural network here.
During encoder phase, it needs to incorporate all information about the input sentence. It
cannot forget the ﬁrst words towards the end of the sentence. During the decoder phase, not
only does it need to have enough information to predict each next word, there also needs to be
some accounting for what part of the input sentence has been already translated, and what still
needs to be covered.
In practice, the proposed models works reasonable well for short sentences (up to, say,
– words), but fails for long sentences. Some minor reﬁnements to this model have been
proposed, such using the sentence embedding state as input to all hidden states of the decoder
phase of the model. This makes the decoder structurally different from the encoder and reduces
some of the load from the hidden state during decoding, since it does not need to remember
anymore the input. Another idea is to reverse the order of the output sentence, so that the last
words of the input sentences are close to the last words of the output sentence.
However, in the following section, we will embark on a more signiﬁcant improvement of
the model, by explicitly modelling alignment of output words to input words.
Adding an Alignment Model
At the time of writing, the state of the art in neural machine translation is a sequence-to-
sequence encoder-decoder model with attention. That is a mouthful, but it is essentially the
model we just described in the previous section, with a explicitly alignment mechanism. In the
deep learning world, this alignment is called attention, we are using the words alignment and
attention interchangeable here.
Since the attention mechanism does add a bit of complexity to the model, we are now slowly
building up to it, by ﬁrst taking a look at the encoder, then the decoder, and ﬁnally the attention
mechanism.
Encoder
The task of the encoder is to provide a representation of the input sentence. The input sentence
is a sequence of words, for which we ﬁrst consult the embedding matrix. Then, as in the basic
language model described previously, we process these words with a recurrent neural network.
This results in hidden states that encode each word with its left context, i.e., all the preceding
words. To also get the right context, we also build a recurrent neural network that runs right-
to-left, or more precisely, from the end of the sentence to the beginning.
Figure . illustrates the model. Having two recurrent neural networks running in two
directions is called a bidirectional recurrent neural network. Mathematically, the encoder
consists of the embedding lookup for each input word xj, and the mapping that steps through
the hidden states ←−
hj and ←−
hj
hj = f(←−−
hj+, ¯E xj)
hj = f(−−→
hj−, ¯E xj)

.. NEURAL TRANSLATION MODELS
Input Word
Embeddings
Left-to-Right
Recurrent NN
Right-to-Left
Recurrent NN
Figure .: Neural machine translation model, part : input encoder. It consists of two recurrent
neural networks, running right to left and left to right (bidrectional recurrent neural network). The
encoder states are the combination of the two hidden states of the recurrent neural networks.
In the equation above, we used a generic function f for a cell in the recurrent neural net-
work. This function may be a typical feed-forward neural network layer — such as f(x) =
tanh(Ax + b) — or the more complex gated recurrent units (GRUs) or long short term memory
cells (LSTMs). The original paper proposing this approached used GRUs, but lately LSTMs
have become more popular.
Note that we could train these models by adding a step that predicts the next word in the
sequence, but we are actually training it in the context of the full machine translation model.
Limiting the description to the decoder, its output is a sequence of word representations that
concatenate the two hidden states (←−
hj, −→
hj).
Decoder
The decoder is also a recurrent neural network. It takes some representation of the input con-
text (more on that in the next section on the attention mechanism) and the previous hidden
state and output word prediction, and generates a new hidden decoder state and a new output
word prediction. See Figure . for an illustration.
Mathematically, we start with the recurrent neural network that maintains a sequence of
hidden states si which are computed from the previous hidden state si−, the embedding of
the previous output word Eyi−, and the input context ci (which we still have to deﬁne).
si = f(si−, Eyi−, ci)
Again, there are several choices for the function f that combines these inputs to generate
the next hidden state: linear transforms with activation function, GRUs, LSTMs, etc. Typically,
the choice here matches the encoder. So, if we use LSTMs for the encoder, then we also use
LSTMs for the decoder.
From the hidden state. we now predict the output word. This prediction takes the form of
a probability distribution over the entire output vocabulary. If we have a vocabulary of, say,
, words, then the prediction is a , dimensional vector, each element corresponding
to the probability predicted for one word in the vocabulary.

CHAPTER . NEURAL MACHINE TRANSLATION
Context
ci
ci-
State
si
si-
Word
ti-
ti
Prediction
Selected
yi-
Word
yi
Eyi-
Eyi
Embedding
Figure .: Neural machine translation model, part : output decoder. Given the context from the
input sentence, and the embedding of the previously selected word, new decoder states and word pre-
dictions are computed.
The prediction vector ti is conditioned on the decoder hidden state si− and, again, the
embedding of the previous output word Eyi− and the input context ci.
ti = softmax
W(Usi− + V Eyi− + Cci)
Note that we repeat the conditioning on Eyi− since we use the hidden state si− and not s.
This separates the encoder state progression from si− to si from the prediction of the output
word ti.
The softmax is used to convert the raw vector into a probability distribution, where the sum
of all values is . Typically, the highest value in the vector indicates the output word token yi.
Its word embedding Eyi− informs the next time step of the recurrent neural network.
During training, the correct output word yi is known, so training proceeds with that word.
The training objective is to give as much probability mass as possible to the correct output
word. The cost function that drives training is hence the negative log of the probability given
to the correct word translation.
cost = −log ti[yi]
Ideally, we want to give the correct word the probability , which would mean a negative
log probability of , but typically it is a lower probability, hence a higher cost. Note that the
cost function is tied to individual words, the overall sentence cost is the sum of all word costs.
During inference on a new test sentence, we typically chose the word yi with the highest
value in ti use its embedding Eyi for the next steps. But we will also explore beam search strate-
gies where the next likely words are selected as yi, creating a different conditioning context for
the next words. More on that later.

.. NEURAL TRANSLATION MODELS
Encoder States
Attention
Input Context
Hidden State
Output Words
Figure .: Neural machine translation model, part : attention model. Associations are computed
between the last hidden state of the decoder and the word representations (encoder states). These asso-
ciations are used to compute a weighted sum of encoder states.
Attention Mechanism
We currently have two loose ends. The decoder gave us a sequence of word representations
hj = (←−
hj, −→
hj) and the decoder expects a context ci at each step i. We now describe the attention
mechanism that ties these ends together.
The attention mechamism is hard to visualize using our typical neural network graphs,
but Figure . gives at least an idea what the input and output relations are. The attention
mechanism is informed by all input word representations (←−
hj, −→
hj) and the previous hidden
state of the decoder si−, and it produces a context state ci.
The motivation is that we want to compute an association between the decoder state (which
contains information where we are in the output sentence production) and each input word.
Based on how strong this association is, or in other words how relevant each particular input
word is to produce the next output word, we want to weight the impact of its word represen-
tation.
Mathematically, we ﬁrst compute this association with a feedforward layer (using weight
vectors wa, ua and bias value ba)
a(si−, hj) = waT si− + uaT hj + ba
The output of this computation is a scalar value, indicating how important input word j is to
produce output word i.
We normalize this attention value, so that the attention values across all input words j add
up to one, using the softmax.
αij =
exp(a(si−, hj))
k exp(a(si−, hk))
Now we use the normalized attention value to weigh the contribution of the input word
representation hj to the context vector ci and we are done.
αijhj
ci =

CHAPTER . NEURAL MACHINE TRANSLATION
Simply adding up word representation vectors (weighted or not) may at ﬁrst seem an odd
and simplistic thing to do. But it is very common practice in deep learning for natural language
processing. Researchers have no qualms about using sentence embeddings that are simply the
sum of word embeddings and other such schemes.
Training
With the complete model in hand, we can now take a closer look at training. One challenge is
that the number of steps in the decoder and the number of steps in the encoder varies with each
training example. Sentence pairs consist of sentences of different length, so we cannot have the
same computation graph for each training example but instead have to dynamically create the
computation graph for each of them. This technique is called unrolling the recurrent neural
networks, and we already discussed it with regard to language models (recall Section ..).
The fully unrolled computation graph for a short sentence pair is shown in Figure ..
Note a couple of things. The error computed from this one sentence pair is the sum of the
errors computed for each word. When proceeding to the next word prediction, we use the
correct word as conditioning context for the decoder hidden state and the word prediction.
Hence, the training objective is based on the probability mass given to the correct word, given
a perfect context. There have been some attempts to use different training objectives, such as
the BLEU score, but they have not yet been shown to be superior.
Practical training of neural machine translation models requires GPUs which are well suited
to the high degree of parallelism inherent in these deep learning models (just think of the many
matrix multiplications). To increase parallelism even more, we process several sentence pairs
(say, ) at once. This implies that we increase the dimensionality of all the state tensors.
To given an example. We represent each input word in speciﬁc sentence pair with a vector
hj. Since we already have a sequence of input words, these are lined up in a matrix. When we
process a batch of sentence pairs, we again line up these matrices into a -dimensional tensor.
Similarly, to give another example, the decoder hidden state si is a vector for each output
word. Since we process a batch of sentences, we line up their hidden states into a matrix. Note
that in this case it is not helpful to line up the states for all the output words, since the states
are computed sequentially.
Recall the ﬁrst computation of the attention mechanism
a(si−, hj) = W asi− + U ahj + ba
We can pass this computation to the GPU with a matrix of encoder states si− and a -
dimensional tensor of input encodings hj, resulting in a matrix of attention values (one dimen-
sion for the sentence pairs, one dimension for the input words). Due to the massive re-use of
values in W a, U a, and ba as well as the inherent parallelism of this computation, GPUs can
show their true power.
You may feel that we just created a glaring contradiction. First, we argued that we have to
process one training example at a time, since sentence pairs typically have different length, and

.. NEURAL TRANSLATION MODELS
<s>
the
house
is
big
</s>
Input Word
Embeddings
Left-to-Right
Recurrent NN
Right-to-Left
Recurrent NN
Attention
Input Context
Hidden State
Output Word
Predictions
Error
Given
Output Words
Output Word
Embedding
<s>
das
Haus
ist
groß
</s>
Figure .: Fully unrolled computation graph for training example with  input tokens <s> the house
is big </s> and  output tokens das Haus is groß</s>. The cost function (error) is computed for each
output word individually, and summed up across the sentence. When walking through the deocder
states, the correct previous output words are used as conditioning context.

CHAPTER . NEURAL MACHINE TRANSLATION
Figure .: To make better use of parallelism in GPUs, we process a batch of training examples (sen-
tence pairs) at a time. Converting a batch of training examples into a set of mini batches that have similar
length. This wastes less computation on ﬁller words (light blue).
hence computation graphs have different size. Then, we argued for batching, say,  sentence
pairs together to better exploit parallelism. These are indeed conﬂicting goals.
See Figure .. When batching training examples together, we have to consider the maxi-
mum sizes for input and output sentences in a batch and unroll the computation graph to these
maximum sizes. For shorter sentences, we ﬁll the remaining gaps with non-words and keep
track of where the valid data is with a mask. This means, for instance, that we have to ensure
that no attention is given to words beyond the length of the input sentence, and no errors and
gradient updates are computed from output words beyond the length of the output sentence.
To avoid wasted computations on gaps, a nice trick is to sort the sentence pairs in the batch
by length and break it up into mini-batches of similar length.
To summarize, training consists of the following steps
Shufﬂe the training corpus (to avoid undue biases due to temporal or topical order)
Break up the corpus into maxi-batches
Break up each maxi-batch into mini-batches
Process each mini-batch, gather gradients
Apply all gradients for a maxi-batch to update the parameters
Typically, training neural machine translation models takes about – epochs (passes through
entire training corpus). A common stopping criteria is to check progress of the model on a val-
idation set (that is not part of the training data) and halt when the error on the validation set
does not improve. Training longer would not lead to any further improvements and may even
degrade performance due to overﬁtting.
Beam Search
Translating with neural translation models proceeds one step at a time. At each step, we predict
one output word. In our model, we ﬁrst compute a probability distribution over all words.
There is a bit of confusion of the technical terms here. Sometimes, the entire training corpus is called a batch, as
used in the contrast between batch updating and online updating. In that context, smaller batches with a subset of
the are called mini-batches (recall Section .. on page ). Here, we use the term batch (or maxi-batch) for such a
subset, and mini-batch for a subset of the subset.

.. NEURAL TRANSLATION MODELS
the
yi
Eyi
ci-
ci
Context
cat
this
State
si-
si
of
Word
ti-
ti
fish
Prediction
there
Selected
yi-
Word
yi
dog
these
Eyi-
Eyi
Embedding
Figure .: Elementary decoding step: The model predicts a word prediction probability distribution.
We select the most likely word (the). Its embedding is part of the conditioning context for the next word
prediction (and decoder state).
We then pick the most likely word and move to the next prediction step. Since the model is
conditioned on the previous output word (recall Equation .), we use its word embedding
in the conditioning context for the next step.
See Figure . for an illustration. At each time step, we obtain a probability distribution
over words. In practice, this distribution is most often quite spiked, only few words — or
maybe even just one word — amass almost all of the probability. In the example, the word the
received the highest probability, so we pick it as the output word.
A real example of how a neural machine translation model translates a German sentence
into English is shown in Figure .. The model tends to give most, if not almost all, proba-
bility mass to the top choice, but the sentence translation also indicates word choice ambiguity,
such as believe (.%) vs. think (.%) or different (.%) vs. various (.%). There is also
ambiguity about grammatical structure, such as if the sentence should start with the discourse
connective but (.%) or the subject I (.%).
This process suggests that we perform -best greedy search. This makes us vulnerable to the
so-called garden-path problem. Sometimes we follow a sequence of words and realize too late
that we made a mistake early on. In that case, the best sequence consists of less probable words
initially which are redeemed by subsequent words in the context of the full output. Consider
the case of having to produce an idiomatic phrase that is non-compositional. The ﬁrst words of
these phrases may be really odd word choices by themselves (e.g., piece of cake for easy). Only
once the full phrase is formed, their choice is redeemed.
Note that we are faced with the same problem in traditional statistical machine translation
models — arguable even more so there since we rely on sparser contexts when making pre-
dictions for the next words. Decoding algorithms for these models keep a list of the n-best
candidate hypotheses, expand them and keep the n-best expanded hypotheses. We can do the
same for neural translation models.

CHAPTER . NEURAL MACHINE TRANSLATION
Input Sentence
ich glaube aber auch , er ist clever genug um seine Aussagen vage genug zu halten , so dass sie auf verschiedene
Art und Weise interpretiert werden können .
Output Word Predictions
Best
Alternatives
but
however (.%), I (.%), yet (.%), and (.%), nor (.%), ...
also (.%), , (.%), it (.%), in (.%), nor (.%), he (.%), ...
also
think (.%), do (.%), believe (.%), , (.%), too (.%), ...
believe
think (.%), feel (.%), do (.%), ...
he
that (.%), it (.%), him (.%), ...
is
’s (.%), has (.%), was (.%), ...
clever
smart (.%), ...
enough
to
about (.%), for (.%), in (.%), of (.%), around (.%), ...
keep
maintain (.%), hold (.%), be (.%), have (.%), make (.%), ...
his
its (.%), statements (.%), what (.%), out (.%), the (.%), ...
statements
testimony (.%), messages (.%), comments (.%), ...
vague
v@@ (.%), in (.%), ambiguous (.%), ...
enough
and (.%), ...
so
, (.%), to (.%), in (.%), and (.%), just (.%), that (.%), ...
they
that (.%), it (.%), can (.%), you (.%), we (.%), to (.%), ...
can
may (.%), could (.%), are (.%), will (.%), might (.%), ...
be
have (.%), interpret (.%), get (.%), ...
interpreted
interpre@@ (.%), constru@@ (.%), ...
in
on (.%), differently (.%), as (.%), to (.%), for (.%), by (.%), ...
different
a (.%), various (.%), several (.%), ways (.%), some (.%), ...
ways
way (.%), manner (.%), ...
</S> (.%), , (.%), ...
</s>
Figure .: Word predictions of the neural machine translation model. Frequently, most of the proba-
bility mass is given to the top choice, but semantically related words may rank high, e.g., believe (.%)
vs. think (.%). The subword units interpre@@ are explain in Section .. on page .

.. NEURAL TRANSLATION MODELS
cat
the
yi
the
cat
this
cat
this
these
cats
of
fish
dog
there
cats
dog
these
Figure .: Beam search in neural machine translation. After committing to a short list of speciﬁc
output words (the beam), new word predictions are made for each. These differ since the committed
output word is part of the conditioning context to make predictions.
When predicting the ﬁrst word of the output sentence, we keep a beam of the top n most
likely word choices. They are scored by their probability. Then, we use each of these words
in the beam in the conditioning context for the next word. Due to this conditioning, we make
different word predictions for each. We now multiply the score for the partial translation (at
this point just the probability for the ﬁrst word), and the probabilities from its word predictions.
We select the highest scoring word pairs for the next beam. See Figure . for an illustration.
This process continues. At each time step, we accumulate word translation probabilities,
giving us scores for each hypothesis. A sentence translation is complete, when the end of
sentence token is produced. At this point, we remove the completed hypothesis from the beam
and reduce beam size by . Search terminates, when no hypotheses are left in the beam.
Search produces a graph of hypotheses, as shown in Figure .. It starts with the start of
sentence symbol <s> and its paths terminate with the end of sentence symbol </s>. Given the
compete graph, the resulting translations can be obtained by following the back-pointers. The
complete hypothesis (i.e., one that ended with a </s> symbol) with the highest score points to
the best translation.
When choosing among the best paths, we score each with the product of its word prediction
probabilities. In practice, we get better results when we normalize the score by the output
length of a translation, i.e., divide by the number of words. We carry out this normalization
after search is completed. During search, all translations in a beam have the same length, so
the normalization would make no difference.
Note that in traditional statistical machine translation, we were able to combine hypothe-
ses if they share the same conditioning context for future feature functions. This not possible
anymore for recurrent neural networks since we condition on the entire output word sequence
from the beginning. As a consequence, the search graph is generally less diverse than search

CHAPTER . NEURAL MACHINE TRANSLATION
<s>
</s>
</s>
</s>
</s>
</s>
</s>
Figure .: Search graph for beam search decoding in neural translation models. At each time step,
the n =  best partial translations (called hypotheses) are selected. An output sentence is complete
when the end of sentence token </s> is predicted. We reduce the beam after that and terminate when
n full sentence translations are completed. Following the back-pointers from the end of sentence tokens
allows us to read them off. Empty boxes represent hypotheses that are not part of any complete path.
graphs in statistical machine translation models. It is really just a search tree where the number
of complete paths is the same as the size of the beam.
Further Readings
The attention model has its roots in a sequence-to-sequence model. Cho et al.
() use recurrent neural networks for the approach. Sutskever et al. () use a LSTM (long short-
term memory) network and reverse the order of the source sentence before decoding.
The seminal work by Bahdanau et al. () adds an alignment model (so called “attention mech-
anism") to link generated output words to source words, which includes conditioning on the hidden
state that produced the preceding target word. Source words are represented by the two hidden states
of recurrent neural networks that process the source sentence left-to-right and right-to-left. Luong et al.
(b) propose variants to the attention mechanism (which they call “global" attention model) and also
a hard-constraint attention model (“local" attention model) which is restricted to a Gaussian distribution
around a speciﬁc input word.
To explicitly model the trade-off between source context (the input words) and target context (the
already produced target words), Tu et al. (a) introduce an interpolation weight (called “context
gate") that scales the impact of the (a) source context state and (b) the previous hidden state and the last
word when predicting the next hidden state in the decoder.
Tu et al. () augment the attention model with a reconstruction step. The generated output is
translated back into the input language and the training objective is extended to not only include the
likelihood of the target sentence but also the likelihood to the reconstructed input sentence.
Reﬁnements
The previous section gave a comprehensive description of the currently most commonly used
basic neural translation model architecture. It performs fairly well out of the box for many

.. REFINEMENTS
Checkpoint ensemble
Multi-run ensemble
Figure .: Two methods to generate alternative systems for ensembling: Checkpoint ensembling uses
model dumps from various stages of the training process, while multi-run ensembling starts indepen-
dent training runs with different initial weights and order of the training data.
language pairs. Since its conception, a number of reﬁnements have been proposed. We will
describe them in this section.
Some of the reﬁnements are fairly general, some target particular use cases or data condi-
tions. To given one example, the best performing system at the recent WMT  evaluation
campaign used ensemble decoding (Section ..), byte pair encoding to address large vo-
cabularies (Section ..), added synthetic data derived from monolingual target side data
(Section ..) , and used deeper models (Section ..).
Ensemble Decoding
A common technique in machine learning is to not just build one system for your problem,
but multiple ones and then combine them. This is called an ensemble of systems. It is such a
successful strategy that various methods have been proposed to systematically build alterna-
tive systems, for instance by using different features or different subsets of the data. For neural
networks, one straightforward way is to use different initializations or stop at different points
in the training process.
Why does it work? The intuitive argument is that each system makes different mistakes.
When two systems agree, then they are more likely both right, rather than both make the same
mistake. One can also see the general principle at play in human behavior, such as setting up
committees to make decisions or the democratic voting in elections.
Applying ensemble methods to our case of neural machine translation, we have to address
two sub-problems: () generating alternate systems, and () combining their output.
Generating Alternative Systems
See Figure . for an illustration of two methods for the ﬁrst sub-problem, generating alter-
native system. When training a neural translation model, we iterate through the training data
until some stopping criteria is met. This is typically a lack of improvements of the cost function
applied to a validation set (measured in cross-entropy), or the translation performance on that
validation set (measured in BLEU).
During training, we dump out the model at ﬁxed intervals (say, every , iteration of
batch processing). Once training is completed, we can look back at the performance of the

CHAPTER . NEURAL MACHINE TRANSLATION
Model
Model
Model
Model
Model
Average
the
cat
this
of
fish
there
dog
these
Figure .: Combining predictions from a ensemble of models: Each model independently predicts a
probability distribution over output words, which are averaged into a combined distribution.
model these different stages. We then pick the, say,  models with the best performance (typ-
ically translation quality measured in BLEU). This is called checkpoint ensembling since we
select the models at different checkpoints in the training process.
Multi-run ensembling requires building systems in completely different training runs. As
mentioned before, this can be accomplished by using different random initialization of weights,
which leads training to seek out different local optima. We also randomly shufﬂe the training
data, so using different random order will also lead to different training outcomes.
Multi-run ensembling usually works a good deal better, but it is also computationally much
more expensive. Note that multi-run ensembling can also build on checkpoint ensembling.
Instead of combining the end points of training, we ﬁrst apply checkpoint ensembling to each
run, and then combine those ensembles.
Combine System Output
Neural translation models allow the combination of several systems fairly deeply. Recall that
the model ﬁrst predicts a probability distribution over possible output words, and then com-
mits to one of the words. This is where we combine the different trained models. Each model
predicts a probability distribution and we then combine their predictions. The combination is
done by simple averaging over the distributions. The averaged distribution is then the basis
for selecting an output word.
See Figure . for an illustration. There may be some beneﬁt to weighing the different
systems differently, although in our way of generating them, they will all have very similar
quality, so this is not typically done.

.. REFINEMENTS
Reranking with Right-to-Left Decoding
One more tweak on the idea of ensembling: Instead of building multiple systems with different
random initialization, we can also build one set of system as before, and then a second set of
system where we reverse the order of the output sentences. The second set of systems are
called right-to-left systems, although arguably this is not a good name since it makes no sense
for languages such as Arabic or Hebrew where the normal writing order is right to left.
The deep integration we described just above does not work anymore for the combination
of left-to-right and right-to-left systems, since they produce output in different order. So, we
have to resort to reranking. This involves several steps:
Use an ensemble of left-to-right systems to generate an n-best list of candidate transla-
tions for each input sentence.
Score each candidate translation with the individual left-to-right and right-to-left sys-
tems.
Combine the scores (simple average) of the different models for each candidate, select the
candidate with the best score for each input sentence.
Scoring a given candidate translation with a right-to-left system does require the require
forced decoding, a special mode of running inference on an input sentence, but predicting a
given output sentence. This mode is actually much closer to training (where also an output
translation is given) the regular inference.
Large Vocabularies
Zipf’s law tells us that words in a language are very unevenly distribution. So, there is always a
large tail of rare words. New words come into the language all the time (e.g., retweeting, website,
woke), and we also have to deal with a very large inventory of names, including company names
(e.g., eBay, Yahoo, Microsoft).
On the other hand, neural methods are not well equipped to deal with such large vocabu-
laries. The ideal representations for neural networks are continuous space vectors. This is why
we ﬁrst convert discrete objects such as words into such word embeddings.
However, ultimately the discrete nature of words shows up. On the input side, we need
to train an embedding matrix that maps each word into its embedding. On the output side
we predict a probability distribution over all output words. The latter is generally the bigger
concern, since the amount of computation involved is linear with the size of the vocabulary,
making this a very large matrix operation.
Hence, neural translation models typically restrict the vocabulary to, say, , to ,
words. In initial work on neural machine translation, only the most frequent words were used,
and all others represented by a unknown or other tag. The translation of these rare words was
handled with a back-off dictionary.

CHAPTER . NEURAL MACHINE TRANSLATION
Obama receives Net@@ any@@ ahu
the relationship between Obama and Net@@ any@@ ahu is not exactly friendly . the two wanted
to talk about the implementation of the international agreement and about Teheran ’s destabil@@
ising activities in the Middle East . the meeting was also planned to cover the conﬂict with the
Palestinians and the disputed two state solution . relations between Obama and Net@@ any@@
ahu have been stra@@ ined for years . Washington critic@@ ises the continuous building of
settlements in Israel and acc@@ uses Net@@ any@@ ahu of a lack of initiative in the peace
process . the relationship between the two has further deteriorated because of the deal that Obama
negotiated on Iran ’s atomic programme . in March , at the invitation of the Republic@@ ans
, Net@@ any@@ ahu made a controversial speech to the US Congress , which was partly seen
as an aff@@ ront to Obama . the speech had not been agreed with Obama , who had rejected a
meeting with reference to the election that was at that time im@@ pending in Israel .
Figure .: Byte pair encoding (BPE) applied to English (model used , BPE operations). Word
splits are indicated with @@. Note that the data is also tokenized and true-cased.
The more common approach today is to break up rare words into subword units. This
may seem a bit crude but is actually very similar to standard approaches in statistical machine
translation to handle compounds (recall website → web + site) and morphology (unfollow →
un + follow, convolutions → convolution + s). It is even a decent approach to the problem of
transliteration of names which are traditionally handled by a sub-modular letter translation
component.
A popular method to create an inventory of subword units and legitimate words is byte
pair encoding. This method is trained on the parallel corpus. First, the words in the corpus
are split into characters (marking original spaces with a special space character). Then, the
most frequent pair of characters is merged (in English, this may be t and h into th). This step
is repeated for a ﬁxed given number of times. Each of these steps increases the vocabulary by
one, beyond the original inventory of single characters.
The example mirrors quite well the behavior of the algorithm on real-world data sets. It
starts with grouping together with frequent letter combinations (e+r, t+h, c+h) and then joins
frequent words (the, in, of). At the end of this process, the most frequent words will emerge
as single tokens, while rare words consist of still un-merged subwords. See Figure . for
an example, where subword units are indicated with two “at" symbols (@@). After , byte
pair encoding operations, the vast majority of words are intact, while rarer words are broken up
(e.g., critic@@ ises, destabil@@ ising). Sometimes, the split seem to be morphologically motivated
(e.g., im@@ pending), but mostly they are not (e.g., stra@@ ined). Note also the decomposition of
the relatively rare name Net@@ any@@ ahu.
Further Readings
A signiﬁcant limitation of neural machine translation models is the computa-
tional burden to support very large vocabularies. To avoid this, typically the vocabulary is reduced

.. REFINEMENTS
to a shortlist of, say, , words, and the remaining tokens are replaced with the unknown word to-
ken “UNK". To translate such an unknown word, Luong et al. (c); Jean et al. (a) resort to a
separate dictionary. Arthur et al. () argue that neural translation models are worse for rare words
and interpolate a traditional probabilistic bilingual dictionary with the prediction of the neural machine
translation model. They use the attention mechanism to link each target word to a distribution of source
words and weigh the word translations accordingly.
Source words such as names and numbers may also be directly copied into the target. Gulcehre et al.
() use a so-called switching network to predict either a traditional translation operation or a copying
operation aided by a softmax layer over the source sentence. They preprocess the training data to change
some target words into word positions of copied source words. Similarly, Gu et al. () augment the
word prediction step of the neural translation model to either translate a word or copy a source word.
They observe that the attention mechanism is mostly driven by semantics and the language model in
the case of word translation, but by location in case of copying.
To speed up training, Mi et al. () use traditional statistical machine translation word and phrase
translation models to ﬁlter the target vocabulary for mini batches.
Sennrich et al. (d) split up all words to sub-word units, using character n-gram models and a
segmentation based on the byte pair encoding compression algorithm.
Using Monolingual Data
A key feature of statistical machine translation system are language models, trained on very
large monolingual data set. The larger the language models, the higher translation quality.
Language models trained on up to a trillion words crawled from the general web have been
used. So, it is a surprise that the basic neural translation model does not use any additional
monolingual data, its language model aspect (the conditioning of the previous hidden decoder
state and the previous output) is trained jointly with the translation model aspect (the condi-
tioning on the input context).
Two main ideas have been proposed to improve neural translation models with monolin-
gual data. One is to transform additional monolingual translation into parallel data by syn-
thesizing the missing half of the data, and the other is to integrate a language model as a
component into the neural network architecture.
Back Translation
Language models improve ﬂuency of the output. Using larger amounts of monolingual data
in the target language give the machine more evidence what are common sequences of words
and what are not.
We cannot use monolingual target side data in our neural translation model training, since
it is missing the source side. So, one idea is to just synthesize this data by back translation. See
Figure . for an illustration of the steps involved.
Train a reverse system that translates from the intended target language into the source
language. We typically use the same neural machine translation setup for this as for
our ﬁnal system, just with source and target ﬂipped. But we may use any system, even
traditional phrase-based systems.

CHAPTER . NEURAL MACHINE TRANSLATION
reverse system
ﬁnal system
Figure .: Creating synthetic parallel data from target-side monolingual data: () train a system in
reverse order, () use it to translate target-side monolingual data into the source language, () combine
the generated synthetic parallel data with the true parallel data in ﬁnal system building.
Use the reverse system to translate target side monolingual data, creating a synthetic
parallel corpus.
Combine the generated synthetic parallel data with the true parallel data when building
the ﬁnal system.
There is an open question on how much synthetic parallel data should be used in relation
to the amount of existing true parallel data. Typically, there are magnitudes more monolingual
data available, but we also do not want to drown out the actual real data. Successful applica-
tions of this idea used equal amounts of synthetic and true data. We may also generate much
more synthetic parallel data, but then ensure during training that we process equal amounts of
each by over-sampling the true parallel data.
Adding a Language Model
The other idea is to train a language model as a separate component of the neural translation
model. Gülçehre et al. () ﬁrst train the large language model as a recurrent neural net-
work on all available data, including the target side of the parallel corpus. Then, they add this
language model to the neural translation model. Since both language model and translation
model predict output words, the natural point to connect the two models is joining them at that
output prediction node in the network by concatenating their conditioning contexts.
We expand Equation . to add the hidden state of the neural language model sLM
to the
hidden state of the neural translation model sTM
i , the source context ci and the previous English
word ei−.
ei = g(ci, sTM
i , sLM
i , ei−)
When training the combined model, we leave the parameters of the large neural language
model unchanged, and update only the parameters of the translation model and the combina-
tion layer. The concern is that otherwise the output side of the parallel corpus would overwrite

.. REFINEMENTS
MT
f→e
MT
f←e
Figure .: Round trip training: In addition to training two models f→e and e→f, as done traditionally
on parallel data, we also optimize both models to convert a sentence f into e and then restore it back
into f, using on monolingual data in f. We may add a corresponding round trip starting with e.
the memory of the large monolingual corpus. In other words, the language model would over-
ﬁt to the parallel training data and be less general.
One ﬁnal question remains: How much weight should be given to the translation model
and how much weight should be given to the language model? The above equation considers
them in all instances the same way. But there may be output words for which the translation
model is more relevant (e.g., the translation of content words with distinct meaning) and output
words where the language model is more relevant (e.g., the introduction of relevant function
words for ﬂuency).
The balance of the translation model and the language model can be achieved with the type
of gated units that we encountered in our discussion of the long short-term memory neural
network architecture (Section ..). Such a gated unit may be predicted solely from the lan-
guage model state sLM
and then used as a factor that is multiplied with that language model
state before it is used in the prediction of Equation ..
gateLM
= f(sLM
i )
¯sLM
= gateLM
× sLM
ei = g(ci, sTM
i , ¯sLM
i , ei−)
Round Trip Training
Looking at the backtranslation idea from a strict machine learning perspective, we can see two
learning objectives. There is the objective to learn the transformations given by the parallel
data, as done traditionally. Then, there is the goal to learn how to convert an output lamguage
sentence into the input language, and then back into the output language with the objective to
match the traditional sentence. A good machine translation model should be able to preserve
the meaning of the output language sentence when mapped into the input language and back.
See Figure . for an illustration. There are two machine translation models. One that
translates sentences in the language direction f→e, the other in the opposite direction e→f.
These two systems may be trained with traditional means, using a parallel corpus. We can
also round trip a sentence f ﬁrst through the f→e and then back through e→f
In this scenario, there are two objectives for model training.

CHAPTER . NEURAL MACHINE TRANSLATION
The translation e’ of the given monolingual sentence f should be a valid sentence in the
language e, as measured with a language model LMe(e’).
The reconstruction of the translation e’ back into the original language f should be easy,
as measured with the translation model MTe→f(f|e’)
These two objectives can be used to update model parameters in both translation models
MTf→e and MTe→f.
Typical model update is driven by correct predictions of each word. In this round-trip
scenario, the translation e’ has to be computed ﬁrst, before we can do the usual training of
model MTe→f with the given sentence pair (e’,f). To make better use of the training data, a
n-best list of translations e’, ..., e’n is computed and model updates are computed for each of
them.
We can also update the model MTf→e with monolingual data in language f by scaling up-
dates by the language model cost LMe(e’i) and the forward translation cost MTf→e(e’i|f) for
each of the translations e’i in the n-best list.
To use monolingual data in language e, training is done in the reverse round trip direction.
For details of this idea, refer to Xia et al. ().
Further Readings
Sennrich et al. (c) back-translate the monolingual data into the input lan-
guage and use the obtained synthetic parallel corpus as additional training data. Xia et al. () use
monolingual data in a dual learning setup. Machine translation engines are trained in both directions,
and in addition to regular model training from parallel data, monolingual data is translated in a round
trip (e to f to e) and evaluated with a language model for language f and reconstruction match back to e
as cost function to drive gradient descent updates to the model.
Deep Models
Learning the lessons from other research ﬁelds such as vision or speech recognition, recent
work in machine translation has also looked at deeper models.
Simply put, this involves
adding more intermediate layers into the baseline architecture.
The core components of neural machine translation are the encoder that takes input words
and converts them into a sequence of contextualized representations and the decoder that gen-
erates a output sequence of words. Both are recurrent neural networks.
Recall that we already discussed how to build deeper recurrent neural networks for lan-
guage modelling (refer back to Section .. on page ). We now extend these ideas to the
recurrent neural networks in the encoder and the decoder.
What all these recurrent neural networks have in common is that they process an input
sequence into an output sequence, and at each time step t information from a new input xt is
combined with the hidden state from the previous time step ht− to predict a new hidden state
ht. From that hidden state additional predictions may be made (output words yt in the case of
the decoder, the next word in the sequence in the case of language models), or the hidden state
is used otherwise (via the attention mechanism in case of the encoder).

.. REFINEMENTS
Context
Decoder State: Stack , Transition
Decoder State: Stack , Transition
Decoder State: Stack , Transition
Decoder State: Stack , Transition
Figure .: Deep Decoder: Instead of a single recurrent neural network (RNN) layer for the decoder
state, in a deep model, it consists of several layers. The illustrations shows a combination of a deep
transition and stacked RNNs. It omits the word prediction, word selection and output word embedding
steps which are identical to the original architecture, shown in Figure . on page .
Decoder
See Figure . for part of the decoder in neural machine translation, using a par-
ticular deeper architecture. We see that instead of a single hidden state ht for a given time step
t, we now have a sequence of hidden states ht,, ht,, ..., ht,I for a given time step t.
There are various options how the hidden states may be connected. Previously, in Sec-
tion .. we presented two ideas. () In stacked recurrent neural networks where a hidden
state ht,i is conditioned on the hidden state from a previous layer ht,i− and the hidden state
at the same depth from a previous time step ht−,i. () In deep transition recurrent neural net-
works, the ﬁrst hidden state ht, is conditioned on the last hidden state from the previous time
step ht−,I and the input, while the other hidden layers ht,I (i > ) are just conditioned on the
previous previous layer ht,i−.
Figure . combines these two ideas. some layers are both stacked (conditioned on the
previous time step ht−,I and previous layer ht,i−), while others are deep transitions (condi-
tioned only on the previous layer ht,i−.
Mathematically, we can break this out into the stacked layers ht,i:
ht, = f(xt, ht−,)
ht,i = fi(ht,i−, ht−,i)
for i >
and the deep transition layers vi,i,j.
vt,i, = gi,(int,i, ht−,i)
int,i is either xt or ht,i−
vt,i,j = gi,j(vt,i,j−)
for j >
ht,i = vt,i,J

CHAPTER . NEURAL MACHINE TRANSLATION
Input Word Embedding
Encoder Layer : LR
Encoder Layer : RL
Encoder Layer : LR
Encoder Layer : RL
Figure .: Deep alternating encoder: combination of the idea of a bidirectional recurrent neural
network previously proposed for neural machine translation (recall Figure . on page ) and the
stacked recurrent neural network (recall Figure . on page ). This architecture may be further
extended with the idea of deep transitions, as shown for the decoder (previous Figure .).
The function fi(ht,i−, ht−,i) is computed as a sequence of function calls gi,j. Each of the
functions gi,j may be implemented as feed-forward neural network layer (matrix multiplication
plus activation function), long-short term memory cell (LSTM), or gated recurrent unit (GRU).
On either case, each function gi,j has its own set of trainable model parameters.
Encoder
Deep recurrent neural networks for the encoder may draw in the same ideas as the
decoder, with one addition: in the baseline neural translation model, we used bidirectional
recurrent neural networks to condition on both left and right context. We want to do the same
for any deep version of the encoder.
Figure . shows one idea how this could be done, called alternating recurrent neural
network. It looks basically like a stacked recurrent neural network, with one twist: the hidden
states at each layer ht,i are alternately conditioned on the hidden state from the previous time
step ht−,i or the next time step ht+,i.
Mathematically, we formulate this as even numbered hidden states ht,i being conditioned
on the left context ht−,i and odd numbered hidden states ht,i+ conditioned on the right
context ht+,i.
ht, = f(xt, ht−,)
ht,i = f(ht,i−, ht−,i)
ht,i+ = f(ht,i, ht+,i+)
As before in the encoder, we can extend this idea by having deep transitions.
Note that deep models are typically augmented with direct connections from the input to
the output. In the case of the encoder, this may mean a direct connection from the embedding
to the ﬁnal encoder layer, or connections at each layer that pass the input directly to the output.

.. REFINEMENTS
relations
between
Obama
and
Netanyahu
have
been
for
years
strained
die
Beziehungen
zwischen
Obama
und
Netanjahu

sind
seit
Jahren
angespannt
Figure .: Alignment vs. Attention: In this example, alignment points from traditional word align-
ment methods are shown as squares, and attention states as shaded boxes depending on the alignment
value (shown as percentage). They generally match up well, but note for instance that the prediction of
the output auxiliary verb sind pays attention to the entire verb group have been strained.
Such residual connections help with training. In early stages, the deep architecture can be
skipped. Only when a basic functioning model has been acquired, the deep architecture can
be exploited to enrich it. We typically see the beneﬁts of residual connections in early training
stages (faster initial reduction of model perplexity), and less so as improvement in the ﬁnal
converged model.
Further Readings
Recent work has shown good results with  stacks and  deep transitions each
for encoder and decoder, as well as alternating networks for the encoder (Miceli Barone et al., ).
There are a large number of variations (including the use of skip connections, the choice of LSTM vs.
GRU, number of layers of any type) that still need to be explored empirical for various data conditions.
Guided Alignment Training
The attention mechanism in neural machine translation models is motivated by the need to
align output words to input words. Figure . shows an example of attention weights given
to English input words for each German output word during the translation of a sentence.
The attention values typically match up pretty well with word alignment used in tradi-
tional statistical machine translation, obtained with tools such as GIZA++ or fast-align which
implement variants of the IBM Models.
There are several good uses for word alignments beyond their intrinsic value of improving
the quality of translations. For instance in the next section, we will look at using the attention

CHAPTER . NEURAL MACHINE TRANSLATION
mechanism to explicitly track coverage of the input. We may also want to override preferences
of the neural machine translation model with pre-speciﬁed translations of certain terminology
or expressions such as numbers, dates, or measurements that are better handled by rule-based
components; this requires to know when the neural model is about to translate a speciﬁc source
word. But also the end user may be interested in alignment information, such as translators
using machine translation in a computer aided translation tool may want to check where an
output word originates from.
Hence, instead of trusting the attention mechanism to implicitly acquire the role as word
alignmer, we may enforce this role. The idea is to provide not just the parallel corpus as train-
ing data, but also pre-computed word alignments using traditional means. Such additional
information may even beneﬁt training of models to converge faster or overcome data sparsity
under low resource conditions.
A straightforward way to add such given word alignment to the training process is to not
change the model at all, but to just modify the training objective. Typically, the goal of training
neural machine translation models is to generate the correct output words. We can add to this
goal to also match the given word alignment.
Formally, we assume to have access to an alignment matrix A that speciﬁes alignment points
Aij input words j and output words i in a way that P
j Aij = , i.e., each output word’s
alignment scores add up to . The model estimates attention scores αij that also add up to  for
each output word: P
j αij =  (recall Equation . on page ). The mismatch between given
alignment scores Aij and computed attention scores αij can be measured in several ways, such
as cross entropy
costCE = −
i=
j=
Aij log αij
or mean squared error
costMSE = −
i=
j=
(Aij − αij)
This cost is added to the training objective and may be weighted.
Further Readings
Chen et al. (b); Liu et al. () add supervised word alignment information
(obtained with traditional statistical word alignment methods) to training. They augment the objective
function to also optimize matching of the attention mechanism to the given alignments.
Modeling Coverage
One impressive aspect of neural machine translation models is how well they are able to trans-
late the entire input sentence, even when a lot of reordering is involved. But this as aspect is
not perfect, occasionally the model translates some input words multiple times, and sometimes
it misses to translate them.

.. REFINEMENTS
solve
problem
suggests
fresh
start
in
order
to
the
the
Social
Housing
alliance
um
das
Problem
zu
lösen,
schlägt
das
Unternehmen
der
Gesellschaft
für
soziale
Bildung
vor

Figure .: Example for over-generation and under-generation: the input tokens around Social Housing
are attended too much, leading to hallucinated output words (das Unternehmen, English: the company),
while the end of the sentence a fresh start is not attended and untranslated.
See Figure . for an example. The translation has two ﬂaws related to mis-allocation of
attention. The beginning of the phrase “Social Housing" alliance receives too much attention,
resulting in a faulty translation with hallucinated words: das Unternehmen der Gesellschaft für
soziale Bildung, or the company of the society for social education. At the end of the input sentence,
the phrase a fresh start does not receive any attention and is hence untranslated in the output.
Hence, an obvious idea is to more strictly model coverage. Given the attention model, a
reasonable way to deﬁne coverage is by adding up the attention states. In a complete sentence
translation, we roughly expect that each input word receives a similar amount of attention. If
some input words never receive attention or too much attention that signals a problem with
the translation.
Enforcing Coverage during Inference
We may restrict the enforcing of proper coverage to
the decoder. When considering multiple hypothesis in beam search, then we should discour-
age the ones that pay too much attention to some input words. And, once hypotheses are
completed, we can penalize those that paid only little attention to some of the input. There are
various ways to come up with scoring functions for over-generation and under-generation.

CHAPTER . NEURAL MACHINE TRANSLATION
αi,k
coverage(j) =
coverage(j) −
over-generation = max

coverage(j)

under-generation = min

The use of multiple scoring functions in the decoder is common practice in traditional statis-
tical machine translation. For now, it is not in neural machine translation. A challenge is to give
proper weight to the different scoring functions. If there are only two or three weights, these
can be optimized with grid search over possible values. For more weights, we may borrow
methods such as MERT or MIRA from statistical machine translation.
Coverage Models
The vector that accumulates coverage of input words may be directly used
to inform the attention model. Previously, the attention given to a speciﬁc input word j was
conditioned on the previous state of the decoder si− and the representation of the input word
hj. Now, we also add as conditioning context the accumulated attention given to the word
(compare to Equation . on page ).
a(si−, hj) = W asi− + U ahj + V acoverage(j) + ba
Coverage tracking may also integrated into the training objective. Taking a page from the
guided alignment training (recall the previous Section ..), we augment the training objec-
tive function with a coverage penalty with some weight λ.
( − coverage(j))
P(yi|x) + λ
log
Note that in general, it is problematic to add such additional functions to the learning ob-
jective, since it does distract from the main goal of producing good translations.
Fertility
So far, we described coverage as the need to cover all input words roughly evenly.
However, even the earliest statistical machine translation models considered the fertility of
words, i.e., the number of output words that are generated from each input word. Consider
the English do not construction: most other language do not require an equivalent of do when
negating a verb. Meanwhile, other words are translated into multiple output words. For in-
stance, the German natürlich may be translated as of course, thus generating  output words.
We may augment models of coverage by adding a fertility components that predicts the
number of output words for each input words. Here one example for a model that predicts the
fertility Φj for each input word, and uses it to normalize the coverage statistics.
Φj = Nσ(Wjhj)
coverage(j) =
Φj
αi,k

.. REFINEMENTS
Fertility Φj is predicted with a neural network layer that is conditioned on the input word
representation hj and uses a sigmoid activation function (thus resulting in values from  to ),
which is scaled to a pre-deﬁned maximum fertility of N.
Feature Engineering versus Machine Learning
The work on modeling coverage in neural
machine translation models is a nice example to contrast between the engineering approach
and the belief in generic machine learning techniques. From an engineering perspective, a
good way to improve a system is to analyze its performance, ﬁnd weak points and consider
changes to overcome them. Here, we notice over-generation and under-generation with respect
to the input, and add components to the model to overcome this problem. On the other hand,
proper coverage is one of the features of a good translation that machine learning should be
able to get from the training data. If it is not able to do that, it may need deeper models, more
robust estimation techniques, ways to ﬁght over-ﬁtting or under-ﬁtting, or other adjustments
to give it just the right amount of power needed for the problem.
It is hard to carry out the analysis needed to make generic machine learning adjustments,
given the complexity of a task like machine translation. Still, the argument for deep learning is
that it does not require feature engineering, such as adding coverage models. It remains to be
seen how neural machine translation evolves over the next years, and if it moves more into a
engineering or machine learning direction.
Further Readings
To better model coverage, Tu et al. (b) add coverage states for each input
word by either (a) summing up attention values, scaled by a fertility value predicted from the input
word in context, or (b) learning a coverage update function as a feed-forward neural network layer.
This coverage state is added as additional conditioning context for the prediction of the attention state.
Feng et al. () condition the prediction of the attention state also on the previous context state and also
introduce a coverage state (initialized with the sum of input source embeddings) that aims to subtract
covered words at each step. Similarly, Meng et al. () separate hidden states that keep track of source
coverage and hidden states that keep track of produced output. Cohn et al. () add a number of
biases to model coverage, fertility, and alignment inspired by traditional statistical machine translation
models. They condition the prediction of the attention state on absolute word positions, the attention
state of the previous output word in a limited window, and coverage (added attention state values) over
a limited window. They also add a fertility model and add coverage in the training objective.
Adaptation
Text may differ by style, topic, degree of formality, and so on. A common problem in the
practical development of machine translation systems is that most of the available training data
is different from the data relevant to a chosen use case. For instance, if your goal is to translate
chat room dialogs, you will realize that there is very little translated chat room data available.
There are massive quantities of ofﬁcial publications from international organizations, random
translations crawled from the web, and maybe somewhat relevant movie subtitle translations.

CHAPTER . NEURAL MACHINE TRANSLATION
general training data
initial training
general system
in-domain training data
adaptation
adapted system
Figure .: Online training of neural machine translation models allows a straightforward domain
adaptation method: Having a general domain translation system trained on general-purpose data, a
handful of additional training epochs on in-domain data allows for a domain-adapted system.
This problem is generally framed as a problem of domain adaptation. In the simplest form,
you have one set of data relevant to your use case — the in-domain data — and another set
that is less relevant — the out-of-domain data.
In traditional statistical machine translation, a vast number of methods for domain adap-
tation have been proposed. Models may be interpolated, we may back-off from in-domain
to out-of-domain models, we may over-sample in-domain data during training or sub-sample
out-of-domain data, etc.
For neural machine translation, a fairly straightforward method is currently the most pop-
ular (see Figure .). This method divides training up into two stages. First, we train the
model on all available data until convergence. Then, we run a few more iterations of training
on the in-domain data only and stop training when performance on the in-domain validate set
peaks. This way, the ﬁnal model beneﬁts from all the training data, but is still specialized to the
in-domain data.
Practical experience with this method shows that the second in-domain training stage may
converge very quickly. The amount if in-domain data is typically relatively small, and only a
handful of training epochs are needed.
Another, less commonly used method draws on the idea of ensemble decoding (Section ..).
If we train separate models on different sets of data, we may combine their predictions, just as
we did for ensemble decoding. In this case, we do want to choose weights for each model,
although how to choose these weights is not a trivial task. If there is just an in-domain and
out-of-domain model, however, this may be simply done by line search over possible values.
Let us now look at a few special cases that arise in practical use.
Subsample in-domain data from large collections
A common problem is that the amount
of available in-domain data is very small, so just training on this data, even in a secondary
adaptation stage, risks overﬁtting — very good performance on the seen data but poor perfor-
mance on everything else.

.. REFINEMENTS
Large random collections of parallel text often contain data that closely matches the in-
domain data. So, we may want to extract this in-domain data from the large collections of
mainly out-of-domain data. The general idea behind a variety of methods is to build two
detectors: one in-domain detector trained on in-domain data, and one out-of-domain detector
trained on out-of-domain data. We then score each sentence pair in the out-of-domain data
with both detectors and select sentence pairs that are preferred (or judged relatively relevant)
by the in-domain detector.
The classic detectors are language models trained on the source and target side of the in-
domain and out-of-domain data, resulting in a total of  language models: the source side
in-domain model LMin
f , the target side in-domain model LMin
e , the source side out-of-domain
model LMout
f , and the target side out-of-domain model LMout
e . Any given sentence pair from
the out-of-domain data is then scored based on these models:
relevancee,f =

LMin
e (e) − LMout
e (e)

LMin
f (f) − LMout
f (f)

We may use traditional n-gram language models or neural recurrent language models.
Some work suggests to replace open class words (nouns, verbs, adjectives, adverbs) with
part-of-speech tags or word clusters. More sophisticated models not only consider domain-
relevance but noisiness of the training data (e.g., misaligned or mistranslated data). We may
even use in-domain and out-of-domain neural translation models to score sentence pairs in-
stead of source and target side sentences in isolation.
The subsampled data may be used in several ways. We may only train on this data to build
our domain-speciﬁc system. Or, we use it in a secondary adaptation stage as outlined above.
Only monolingual in-domain data
What if we have no parallel data in the domain of our
use case? Two main ideas have been explored. Firstly, we may still use the monolingual data,
may it be in the source or target language or both, for subsampling parallel data from a large
pile of general data, as outline above.
Another idea is to use existing parallel data to train an out-of-domain model, then back-
translate out-of-domain data (recall Section ..) to generate a synthetic in-domain corpus,
and then use this data to adapt the initial model. In traditional statistical machine translation,
much adaptation success has been achieved with just interpolating the language model, and
this idea is the neural translation equivalent to that.
Multiple domains
Sometimes, we have multiple collections of data that are clearly identiﬁed
by domain — typically categories such as information technology, medical, law, etc. We can
use the techniques described above to build specialized translation models for each of these
domains.
For a given test sentence, we then select the appropriate model. If we do not know the
domain of the test sentence, we ﬁrst have to build a classiﬁer that allows us to automatically

CHAPTER . NEURAL MACHINE TRANSLATION
make this determination. The classiﬁer may be based on the methods for domain detectors
described above. Given the decision of the classiﬁer, we then select the most appropriate model.
But we do not have to commit to a single domain. The classiﬁer may instead provide a
distribution of relevance of the speciﬁc domain models (say, % domain A, % domain B,
% domain C) which are then used as weights in an ensemble of domain-speciﬁc models.
The domain classiﬁcation may done based on a whole document instead of each individual
sentence, which brings in more context to make a more robust decision.
As a ﬁnal remark, it is hard to give conclusive advice on how to handle adaptation chal-
lenges, since it is such a broad topic. The style of the text may be more relevant than its content.
Data may differ narrowly (e.g., ofﬁcial publications from the United Nations vs. ofﬁcial an-
nouncements from the European Union) or dramatically (e.g., chat room dialogs vs. published
laws). The amounts of in-domain and out-of-domain data differs. The data may be cleanly
separated by domain or just come in a massive disorganized pile. Some of the data may be of
higher translation quality than other, which may be polluted by noise such as mistranslations,
misalignments, or even generated by some other machine translation system.
Further Readings
There is often a domain mismatch between the bulk (or even all) of the training
data for a translation and its test data during deployment. There is rich literature in traditional statistical
machine translation on this topic. A common approach for neural models is to ﬁrst train on all available
training data, and then run a few iterations on in-domain data only (Luong and Manning, ), as
already pioneered in neural language model adaption (Ter-Sarkisov et al., ). Servan et al. ()
demonstrate the effectiveness of this adaptation method with small in-domain sets consisting of as little
as  sentence pairs.
Chu et al. () argue that given small amount of in-domain data leads to overﬁtting and suggest
to mix in-domain and out-of-domain data during adaption. Freitag and Al-Onaizan () identify
the same problem and suggest to use an ensemble of baseline models and adapted models to avoid
overﬁtting. Peris et al. () consider alternative training methods for the adaptation phase but do
not ﬁnd consistently better results than the traditional gradient descent training. Inspired by domain
adaptation work in statistical machine translation on sub-sampling and sentence weighting, Chen et al.
() build an in-domain vs. out-of-domain classiﬁer for sentence pairs in the training data, and then
use its prediction score to reduce the learning rate for sentence pairs that are out of domain.
Farajian et al. () show that traditional statistical machine translation outperforms neural ma-
chine translation when training general-purpose machine translation systems on a collection data, and
then tested on niche domains. The adaptation technique allows neural machine translation to catch up.
A multi-domain model may be trained and informed at run-time about the domain of the input
sentence. Kobus et al. () apply an idea initially proposed by Sennrich et al. (a) - to augment
input sentences for register with a politeness feature token - to the domain adaptation problem. They
add a domain token to each training and test sentence. Chen et al. (b) report better results over
the token approach to adapt to topics by encoding the given topic membership of each sentence as an
additional input vector to the conditioning context of word prediction layer.

.. REFINEMENTS
Adding Linguistic Annotation
One of the big debates in machine translation research is the question if the key to progress is to
develop better, relatively generic, machine learning methods that implicitly learn the important
features of language, or to use linguistic insight to augment data and models.
Recent work in statistical machine translation has demonstrated the beneﬁts of linguisti-
cally motivated models. The best statistical machine translation systems in major evaluation
campaigns for language pairs such as Chinese–English and German–English are syntax-based.
While they translate sentences, they also build up the syntactic structure of the output sentence.
There have been serious efforts to move towards deeper semantics in machine translation.
The turn towards neural machine translation was at ﬁrst hard swing back towards better
machine learning while ignoring much linguistic insights. Neural machine translation views
translation as a generic sequence to sequence task, which just happens to involve sequences of
words in different languages. Methods such as byte pair encoding or character-based transla-
tion models even put the value of the concept of a word as a basic unit into doubt.
However, recently there have been also attempts to add linguistic annotation into neural
translation models, and steps towards more linguistically motivated models. We will take a
look at successful efforts to integrate () linguistic annotation to the input sentence, () linguistic
annotation to the output sentence, and () build linguistically structured models.
Linguistic annotation of the input
One of the great beneﬁts of neural networks is their abil-
ity to cope with rich context. In the neural machine translation models we presented, each
word prediction is conditioned on the entire input sentence and all previously generated out-
put words. Even if, as it is typically the case, a speciﬁc input sequence and partially generated
output sequence has never been observed before during training, the neural model is able to
generalize the training data and draw from relevant knowledge. In traditional statistical mod-
els, this required carefully chosen independence assumptions and back-off schemes.
So, adding more information to the conditioning context in neural translation models can
be accommodated rather straightforwardly. First, what information would be like to add? The
typical linguistic treasure chest contains part-of-speech tags, lemmas, morphological properties
of words, syntactic phrase structure, syntactic dependencies, and maybe even some semantic
annotation.
All of these can be formatted as annotations to individual input words. Sometimes, this
requires a bit more work, such as syntactic and semantic annotation that spans multiple words.
See Figure . for an example. To just walk through the linguistic annotation of the word girl
in the sentence:
Part of speech is NN, a noun.
Lemma is girl, the same as the surface form. The lemma differs for watched / watch.
Morphology is singular.

CHAPTER . NEURAL MACHINE TRANSLATION
Words
the
girl
watched
attentively
the
beautiful
ﬁreﬂies
Part of speech
DET
NN
ADV
VFIN
DET
JJ
NNS
Lemma
the
girl
watch
attentive
the
beautiful
ﬁreﬂy
Morphology
SING.
PAST
PLURAL
Noun phrase
BEGIN
CONT
OTHER
OTHER
BEGIN
CONT
CONT
Verb phrase
OTHER
OTHER
BEGIN
CONT
CONT
CONT
CONT
Synt. dependency
girl
watched
watched
ﬁreﬂies
ﬁreﬂies
watched
Depend. relation
DET
SUBJ
ADV
DET
ADJ
OBJ
Semantic role
ACTOR
MANNER
MOD
PATIENT
Semantic type
HUMAN
VIEW
ANIMATE
Figure .: Linguistic annotation of a sentence, formatted as word-level factored representation
The word is the continuation (CONT) of the noun phrase that started with the.
The word is not part of a verb phrase (OTHER).
Its syntactic head is watched.
The dependency relationship to the head is subject (SUBJ).
Its semantic role is ACTOR.
There are many schemes of semantic types. For instance girl could be classiﬁed as HU-
MAN.
Note how phrasal annotations are handled. The ﬁrst noun phrase is the girl. It is common
to use an annotation scheme that tags individual words in a phrasal annation as BEGIN and
CONTINUATION (or INTERMEDIATE), while labelling words outside such phrases as OTHER.
How do we encode the word-level factored representation? Recall that words are initially
represented as -hot vectors. We can encode each factor in the factored representation as a -
hot vector. The concatenation of these vectors is then used as input to the word embedding.
Note that mathematically this means, that each factor of the representation is mapped to a
embedding vector, and the ﬁnal word embedding is the sum of the factor embeddings.
Since the input to the neural machine translation system is still a sequence of word embed-
dings, we do not have to change anything in the architecture of the neural machine translation
model. We just provide richer input representations and hope that the model is able to learn
how to take advantage of it.
Coming back to the debate about linguistics versus machine learning. All the linguistic an-
notation proposed here can arguable be learned automatically as part of the word embeddings
(or contextualized word embeddings in the hidden encoder states). This may or may not be
true. But it does provide additional knowledge that comes from the tools that produce the an-
notation and that is particularly relevant if there is not enough training data to automatically
induce it. Also, why make the job harder for the machine learning algorithm than needed? In
other words, why force the machine learning to discover features that can be readily provided?

.. REFINEMENTS
Sentence
the girl watched attentively the beautiful ﬁreﬂies
Syntax tree
VP
NP
ADVP
VFIN
NP
NN
DET
NNS
ADV
JJ
DET
watched
girl
the
ﬁreﬂies
attentively
beautiful
the
Linearized
(S (NP (DET the ) (NN girl ) ) (VP (VFIN watched ) (ADVP (ADV attentively
) ) (NP (DET the ) (JJ beautiful ) (NNS ﬁreﬂies ) ) ) )
Figure .: Linearization of phrase structure grammar tree into a sequence of words — e.g., girl,
watched — and tags — e.g., (S, (NP, )
Ultimately, these questions will be resolved empirically by demonstrating what actually works
in speciﬁc data conditions.
Linguistic annotation of the output
What we have done for input words could be done also
for output words. Instead of discussing the ﬁne points about what adjustments need to made
(e.g., separate softmax for each output factor), let us take a look at another annotation scheme
for the output that has been successfully applied to neural machine translation.
Most syntax-based statistical machine translation models have focused on adding syntax
to the output side. Traditional n-gram language models are good at promoting ﬂuency among
neighboring words, they are not powerful enough to ensure overall grammaticality of each
output sentence. By designing models that also produce and evaluate the syntactic parse struc-
ture for each output sentence, syntax-based models give the means to promote grammatically
correct output.
The word-level annotation of phrase structure syntax suggested in Figure . is rather
crude. The nature of language is recursive, and annotating nested phrases cannot be easily
handled with a BEGIN/CONT/OTHER scheme. Instead, typically tree structures are used to
represent syntax.
See Figure . for an example. It shows the phrase structure syntactic parse tree for our
example sentence The girl watched attentively the beautiful ﬁreﬂies. Generating a tree structures is
generally a quite different process than generating a sequence. It is typically built recursively
bottom-up with algorithms such as chart parsing.
However, we can linearize the parse tree into a sequence of words and structural tokens
that indicate the beginning — e.g., “(NP" — and end — closing parenthesis “)" — of syntactic
phrases. So, forcing syntactic parse tree annotations into our sequence-to-sequence neural ma-
chine translation model may be done by encoding the parse structure with additional output
tokens. To be perfectly clear, the idea is to produce as the output of the neural translation sys-
tem not just a sequence of words, but a sequence of a mix of output words and special tokens.

CHAPTER . NEURAL MACHINE TRANSLATION
The hope is that forcing the neural machine translation model to produce syntactic structure
(even in a linearized form) encourages it to produce syntactically well-formed output. There is
some evidence to support this hope, despite the simplicity of the approach.
Linguistically structured models
The ﬁeld of syntactic parsing has not been left untouched
by the recent wave of neural networks. The previous section suggests that syntactic parsing
may be done as simply as framing it as a sequence to sequence with additional output tokens.
However, the best-performing syntactic parsers use model structures that take the recur-
sive nature of language to heart. They are either inspired by convolutional networks and build
parse trees bottom-up, or are neural versions of left-to-right push-down automata that main-
tain a stack of opened phrases that any new word may extend or close, or be pushed down the
stack to start a new phrase.
There is some early work on integrating syntactic parsing and machine translation into a
uniﬁed framework but no consensus on best practices has emerged yet. At the time of writing,
this is clearly still a challenge for future work.
Further Readings
Wu et al. () propose to use factored representations of words (using lemma,
stem, and part of speech), with each factor encoded in a one-hot vector, in the input to a recurrent
neural network language model. Sennrich and Haddow () use such representations in the input
and output of neural machine translation models, demonstrating better translation quality.
Multiple Language Pairs
There are more than two languages in the world. And we also have training data for many
language pairs, sometimes it is highly overlapping (e.g., European Parliament proceedings in
languages), sometimes it is unique (e.g. Canadian Hansards in French and English). For
some language pairs, a lot of training data is available (e.g., French–English). But for most
language pairs, there is only very little, including commercially interesting language pairs such
as Chinese–German or Japanese–Spanish.
There is a long history of moving beyond speciﬁc languages and encode meaning language-
independent, sometimes called interlingua. In machine translation, the idea is to map the input
language ﬁrst into an interlingua, and then map the interlingua into the output language. In
such a system, we have to build just one mapping step into and one step out of the interlingua
for each language. Then we can translate between it and all the other languages for which we
have done the same.
Researchers in deep learning often do not hesitate to claim that intermediate states in neural
translation models encode semantics or meaning. So, can we train a neural machine translation
system that accepts text in any language as input and translates it into any other language?

.. REFINEMENTS
Multiple Input Languages
Let us say, we have two parallel corpora, one for German–English,
and one for French–English. We can train a neural machine translation model on both corpora
at the same time by simply concatenating them. The input vocabulary contains both German
and French words. Any input sentence will be quickly recognized as being either German or
French, due to the sentence context, disambiguating words such as du (you in German, of in
French).
The combined model trained on both data sets has one advantage over two separate mod-
els. It is exposed to both English sides of the parallel corpora and hence can learn a better
language model. There may be also be general beneﬁts to having diversity in the data, leading
to more robust models.
Multiple Output Languages
We can do the same trick for the output language, by concate-
nating, say, a French–English and a French–Spanish corpus. But given a French input sentence
during inference, how would the system know which output language to generate? A crude
but effective way to signal this to the model is by adding a tag like [SPANISH] as ﬁrst token of
the input sentence.
[ENGLISH] N’y a-t-il pas ici deux poids, deux mesures?
⇒ Is this not a case of double standards?
[SPANISH] N’y a-t-il pas ici deux poids, deux mesures?
⇒ £No puede verse con toda claridad que estamos utilizando un doble rasero?
If we train a system on the three corpora mentioned (German–English, French–English, and
French–Spanish) we can also use it translate a sentence from German to Spanish — without
having ever presented a sentence pair as training data to the system.
[SPANISH] Messen wir hier nicht mit zweierlei Maß?
⇒ £No puede verse con toda claridad que estamos utilizando un doble rasero?
For this to work, there has to be some representation of the meaning of the input sentence
that is not tied to the input language and the output language. Surprisingly, experiments show
that this actually does work, somewhat. To achieve good quality, however, some parallel data
in the desired language pair is needed, but much less than for a standalone model (Johnson
et al., ).
Figure . summarizes this idea. A single neural machine translation is trained on vari-
ous parallel corpora in turn, resulting in a system that may translate between any seen input
and output language. It is likely that increasingly deeper models (recall Section ..) may
better serve as multi-language translators, since their deeper layer compute more abstract rep-
resentations of language.
The idea of marking the output language with a token such as [SPANISH] has been explored
more widely in the context of systems for a single language pair. Such tokens may represent
the domain of the input sentence (Kobus et al., ), or the required level of politeness of the
output sentence (Sennrich et al., a).

CHAPTER . NEURAL MACHINE TRANSLATION
French
German
MT
English
Spanish
Figure .: Multi-language machine translation system trained on one language pair at a time, rotating
through many of them. After training on French–English, French–Spanish, and German–English, it is
even able to translate from German to Spanish.
Sharing Components
Instead of just throwing data at a generic neural machine translation
model, we may want to more carefully consider which components may be shared among
language-pair-speciﬁc models. The idea is to train one model per language pair, but some of
the components are identical in these unique models.
The encoder may be shared in models that have the same input language.
The decoder may be shared in models that have the same output language.
The attention mechanism may be shared in all models for all language pairs.
Sharing components means is that the same parameter values (weight matrices, etc.) are used
in these separate models. Updates to them when training a model for one language pair then
also changes them for in the model for the other language pairs. There is no need to mark the
output language, since each model is trained for a speciﬁc language pair.
The idea of shared training of components can also be pushed further to exploit mono-
lingual data. The encoder may be trained on monolingual input language data, but we will
need to add a training objective (e.g., language model cross-entropy). Also, the decoder may
be trained in isolation with monolingual language model data. However, since there are no
context states available, these have to be blanked out, which may lead it to learn to ignore the
input sentence and function only as a target side language model.
Further Readings
Johnson et al. () explore how well a single canonical neural translation model
is able to learn from multiple to multiple languages, by simultaneously training on on parallel corpora
for several language pairs. They show small beneﬁts for several input languages with the same output
languages, mixed results for translating into multiple output languages (indicated by an additional
input language token). The most interesting result is the ability for such a model to translate in language
directions for which no parallel corpus is provided, thus demonstrating that some interlingual meaning
representation is learned, although less well than using traditional pivot methods.
Firat et al. () support multi-language input and output by training language-speciﬁc encoders
and decoders and a shared attention mechanism.

.. ALTERNATE ARCHITECTURES
Input Word
Embeddings
K Layer
K Layer
L Layer
Figure .: Encoding a sentence with a convolutional neural network. By always using two convolu-
tional layers, the size of the convolutions differ (here K and K). Decoding reverses this process.
Alternate Architectures
Most of neural network research has focused on the use of recurrent neural networks with
attention. But this is by no means the only architecture for neural networks. Arguable, a disad-
vantage of using recurrent neural networks on the input side is that it requires a long sequential
process that consumes each input word in one step. This also prohibits the ability to parallelize
the processing of all words at once, thus limiting the use of the capabilities of GPUs.
There have been a few alternate suggestions for the architecture of neural machine transla-
tion models. We will brieﬂy present some of them in this section. It remains to be seen, if they
are a curiosity or conquer the ﬁeld.
Convolutional Neural Networks
The ﬁrst end-to-end neural machine translation model of the modern era (Kalchbrenner and
Blunsom, ) was actually not based on recurrent neural networks, but based on convolu-
tional neural networks. These had been shown to be very successful in image processing, thus
looking for other applications was a natural next step.
See Figure . for an illustration of a convolutional network that encodes an input sen-
tence. The basic building block of these networks is a convolution. It merges the representation
of i input words into a single representation by using a matrix Ki. Applying the convolution
to every sequence of input words reduces the length of the sentence representation by i − .
Repeating this process leads to a sentence representation in a single vector.
The illustration shows an architecture with two convolutional Ki layers, followed by a ﬁnal
Li layer that merges the sequence of phrasal representations into a single sentence representa-
tion. The size of the convolutional kernels Ki and Li depends on the length of the sentences.
The example shows a -word sentence and a sequence of K, K, and L layers. For longer
sentences, bigger kernels are needed.
The hierarchical process of building up a sentence representation bottom-up is well grounded
in linguistic insight in the recursive nature of language. It is similar to chart parsing, except that
we are not committing to a single hierarchical structure. On the other hand, we are asking an

CHAPTER . NEURAL MACHINE TRANSLATION
Input Word
Embeddings
K Encoding Layer
K Encoding Layer
Transfer Layer
K Decoding Layer
K Decoding Layer
Selected Word
Output Word
Embedding
Figure .: Reﬁnement of the convolutional neural network model. Convolutions do not result in a
single sentence embedding but a sequence. The encoder is also informed by a recurrent neural network
connections from output word embeddings to ﬁnal decoding layer.
awful lot from the resulting sentence embedding to represents the meaning of an entire sen-
tence of arbitrary length.
Generating the output sentence translation reverses the bottom-up process. One problem
for the decoder is to decide the length of the output sentence. One option to address this
problem is to add a model that predicts output length from input length. This then leads to the
selection of the size of the reverse convolution matrices.
See Figure . for an illustration of a variation of this idea. The shown architecture always
uses a K and a K convolutional layer, resulting in a sequence of phrasal representations, not
a single sentence embedding. There is an explicit mapping step from phrasal representations
of input words to phrasal representations of output words, called transfer layer.
The decoder of the model includes a recurrent neural network on the output side. Sneaking
in a recurrent neural network here does undermine a bit the argument about better paralleliza-
tion. However, the claim still holds true for encoding the input, and a sequential language
model is just a too powerful tool to disregard.
While the just-described convolutional neural machine translation model helped to set the
scene for neural network approaches for machine translation, it could not be demonstrated to
achieve competitive results compared to traditional approaches. The compression of the sen-
tence representation into a single vector is especially a problem for long sentences. However,
the model was used successfully in reranking candidate translations generated by traditional
statistical machine translation systems.

.. ALTERNATE ARCHITECTURES
Input Word
Embeddings
Convolution
Layer
Convolution
Layer
Convolution
Layer
Figure .: Encoder using stacked convolutional layers. Any number of layers may be used.
Convolutional Neural Networks With Attention
Gehring et al. () propose an architecture for neural networks that combines the ideas of
convolutional neural networks and the attention mechanism. It is essentially the sequence-to-
sequence attention that we described as the canonical neural machine translation approach, but
with the recurrent neural networks replaced by convolutional layers.
We introduced convolutions in the previous section. The idea is to combine a short sequence
of neighboring words into a single representation. To look at it in another way, a convolution
encodes a word with its left and right context, in a limited window. Let us now describe in
more detail what this means for the encoder and the decoder in the neural model.
Encoder
See Figure . for an illustration of the convolutional layers used in the encoder.
For each input word, the state at each layer is informed by the corresponding state in the pre-
vious layer and its two neighbors. Note that these convolutional layers do not shorten the
sequence, because we have a convolution centered around each word, using padding (vectors
with zero values) for word positions that are out of bounds.
Mathematically, we start with the input word embeddings Exj and progress through a
sequence of layer encodings hd,j at different depth d until a maximum depth D.
h,j = E xj
hd,j = f(hd−,j−k, ..., hd−,j+k)
for d > , d ≤ D
The function f is a feed-forward layer, with a residual connection from the corresponding
previous layer state hd−,j.
Note that even with a few convolutional layers, the ﬁnal representation of a word hD,j may
only be informed by partial sentence context — in contrast to the bi-directional recurrent neural
networks in the canonical model. However, relevant context words in the input sentence that
help with disambiguation may be outside this window.
On the other hand, there are signiﬁcant computational advantages to this idea. All words at
one depth can be processed in parallel, even combined into one massive tensor operation that
can be efﬁciently parallelized on a GPU.

CHAPTER . NEURAL MACHINE TRANSLATION
Input Context
Output Word
Predictions
Decoder
Convolution
Decoder
Convolution
Output Word
Embedding
Selected
Word
Figure .: Decoder in convolutional neural network with attention. The decoder state is computed
as a sequence of convolutional layers (here: ) over the already predicted output words. Each convolu-
tional state is also informed by the input context computed from the input sentence and attention.
Decoder
The decoder in the canonical model also has at its core a recurrent neural network.
Recall its state progression deﬁned in Equation . on page :
si = f(si−, Eyi−, ci)
where si is the encoder state, Eyi− the embedding of the previous output word, and ci the
input context.
The convolutional version of this does not have recurrent decoder states, i.e., the compu-
tation does not depend on the previous state si−, but is conditioned on the sequence of the κ
most recent previous words.
si = f(Eyi−κ, ..., Eyi−, ci)
Furthermore, these decoder convolutions may be stacked, just as the encoder convolutional
layers.
s,i = f(Eyi−κ, ..., Eyi−, ci)
sd,i = f(sd−,i−κ−, ..., sd−,i, ci)
for d > , d ≤ ˆD
See Figure . for an illustration of these equations. The main difference between the
canonical neural machine translation model and this architecture is the conditioning of the
states of the decoder. They are computed in a sequence of convolutional layers, and also always
the input context.
Attention
The attention mechanism is essentially unchanged from the canonical neural trans-
lation model. Recall that is is based on an association a(si−, hj) between the word represen-
tations computed by the encoder hj and the previous state of the decoder si− (refer back to
Equation . on page ).

.. ALTERNATE ARCHITECTURES
Since we still have such encoder and decoder states (hD,j and s ˆD,i−), we use the same
here. These association scores are normalized and used to compute a weighted sum of the
input word embeddings (i.e., the encoder states hD,j). A reﬁnement is that the encoder state
hD,j and the input word embedding xj is combined via addition when computing the context
vector. This is the usual trick of using residual connections to assist training with deep neural
networks.
Self-Attention
The critique of the use of recurrent neural networks is that they require a lengthy walk-through,
word by word, of the entire input sentence, which is time-consuming and limits parallelization.
The previous sections replaced the recurrent neural networks in our canonical model with con-
volutions. However, these have a limited context window to enrich representations of words.
What we would like is some architectural component that allows us to use wide context and
can be highly parallelized. What could that be?
In fact, we already encountered it: the attention mechanism. It considers associations be-
tween every input word and any output word, and uses it to build a vector representation of
the entire input sequence. The idea behind self-attention is to extend this idea to the encoder.
Instead of computing the association between an input and an output word, self-attention com-
putes the association between any input word and any other input word. One way to view it is
that this mechanism reﬁnes the representation of each input word by enriching it with context
words that help to disambiguate it.
Computing Self-Attention
Vaswani et al. () deﬁne self attention for a sequence of vectors
hj (of size |h|), packed into a matrix H, as
|h|
self-attention(H) = softmax
HHT

Let us look at this equation in detail. The association between every word representation
hj any other context word hk is done via the dot product between the packed matrix H and its
transpose HT , resulting in a vector of raw association values HHT . The values in this vector are
ﬁrst scaled by the size of the word representation vectors |h|, and then by the softmax, so that
their values add up to . The resulting vector of normalized association values is then used to
weigh the context words.
Another way to put Equation . without the matrix H notation but using word repre-
sentation vectors hj:
ajk =
|h|
|h|hjhT
raw association
HHT

αjk =
exp(ajk)
κ exp(ajκ)
normalized association (softmax)
αjκhk
weighted sum
self-attention(hj) =

CHAPTER . NEURAL MACHINE TRANSLATION
Self-Attention Layer
The self-attention step described above is only one step in the self-
attention layer used to encode the input sentence. There are four more steps that follow it.
We combine self-attention with residual connections that pass the word representation
through directly
self-attention(hj) + hj
Next up is a layer normalization step (described in Section .. on page ).
ˆhj = layer-normalization(self-attention(hj) + hj)
A standard feed-forward step with ReLU activation function is applied.
relu(Wˆhj + b)
This is also augmented with residual connections and layer normalization.
layer-normalization(relu(Wˆhj + b) + ˆhj)
Taking a page from deep models, we now stack several such layers (say, D = ) on top of
each other.
h,j = Exj
start with input word embedding
hd,j = self-attention-layer(hd−,j)
for d > , d ≤ D
The deep modeling is the reason behind the residual connections in the self-attention layer
— such residual connections help with training since they allow a shortcut to the input which
may be utilized in early stages of training, before it can take advantage of the more complex
interdependencies that deep models enable. The layer normalization step is one standard train-
ing trick that also helps especially with deep models.
Attention in the Decoder
Self-attention is also used in the decoder, now between output
words. The decoder also has more traditional attention. In total there are  sub layers.
Self attention: Output words are initially encoded by word embeddings si = Eyi. We per-
form exactly the same self-attention computation as described in Equation .. How-
ever, the association of a word si is limited to words sk with k ≤ i, i.e., just the previously
produced output words. Let us denote the result of this sub layer for output word i as ˜si
Attention: The attention mechanism in this model follows very closely self-attention. The
only difference is that, previously, we compute self attention between the hidden states H
and themselves. Now, we compute attention between the decoder states ˜S and the ﬁnal
encoder states H.
|h|
attention( ˜S, H) = softmax
˜SHT

.. ALTERNATE ARCHITECTURES
Input Word
Embeddings
Self Attention
Layer
Self Attention
Layer
Decoder
Layer
Decoder
Layer
Output Word
Prediction
Selected
Output Word
Output Word
Embedding
Figure .: Attention-based machine translation model: the input is encoded with several layers of
self-attention. The decoder computes attention-based representations of the input in several layers,
initialized with the previous word embeddings.
Using the same more detailed exposition as above for self-attention:
aik =
|h|
|h| ˜sihT
raw association
˜SHT

αik =
exp(aik)
κ exp(aiκ)
normalized association (softmax)
αjκhk
weighted sum
attention(˜si) =
This attention computation is augmented by adding in residual connections, layer nor-
malization, and an additional ReLU layer, just like the self-attention layer described above.
It is worth noting that, the output of the attention computation is a weighted sum over
input word representations P
k αjκhk. To this, we add the (self-attended) representation
of the decoder state ˜si via a residual connection. This allows skipping over the deep
layers, thus speeding up training.
Feed-forward layer: This sub layer is identical to the encoder, i.e., relu(Wsˆsi + bs)
Each of the sub-layers is followed by the add-and-norm step of ﬁrst using residual connec-
tions and then layer normalization (as noted in the description of the attention sub layer).
The entire model is shown in Figure .,

CHAPTER . NEURAL MACHINE TRANSLATION
Further Readings
Kalchbrenner and Blunsom () build a comprehensive machine translation
model by ﬁrst encoding the source sentence with a convolutional neural network, and then generate
the target sentence by reversing the process. A reﬁnement of this was proposed by Gehring et al. ()
who use multiple convolutional layers in the encoder and the decoder that do not reduce the length of
the encoded sequence but incorporate wider context with each layer.
Vaswani et al. () replace the recurrent neural networks used in attentional sequence-to-sequence
models with multiple self-attention layers, both for the encoder as well as the decoder. There are a
number of additional reﬁnements of this model: so-called multi-head attention, encoding of sentence
positions of words, etc.
Current Challenges
Neural machine translation has emerged as the most promising machine translation approach
in recent years, showing superior performance on public benchmarks (Bojar et al., ) and
rapid adoption in deployments by, e.g., Google (Wu et al., ), Systran (Crego et al., ),
and WIPO (Junczys-Dowmunt et al., ). But there have also been reports of poor perfor-
mance, such as the systems built under low-resource conditions in the DARPA LORELEI pro-
gram.
Here, we examine a number of challenges to neural machine translation and give empirical
results on how well the technology currently holds up, compared to traditional statistical ma-
chine translation. We show that, despite its recent successes, neural machine translation still
has to overcome various challenges, most notably performance out-of-domain and under low
resource conditions.
What a lot of the problems have in common is that the neural translation models do not
show robust behavior when confronted with conditions that differ signiﬁcantly from training
conditions — may it be due to limited exposure to training data, unusual input in case of out-
of-domain test sentences, or unlikely initial word choices in beam search. The solution to these
problems may hence lie in a more general approach of training that steps outside optimizing
single word predictions given perfectly matching prior sequences.
Another challenge that we do not examine empirically: neural machine translation sys-
tems are much less interpretable. The answer to the question of why the training data leads
these systems to decide on speciﬁc word choices during decoding is buried in large matrices
of real-numbered values. There is a clear need to develop better analytics for neural machine
translation.
We use common toolkits for neural machine translation (Nematus) and traditional phrase-
based statistical machine translation (Moses) with common data sets, drawn from WMT and
OPUS. Unless noted otherwise, we use default settings, such as beam search and single model
decoding. The training data is processed with byte-pair encoding (Sennrich et al., c) into
subwords to ﬁt a , word vocabulary limit.
https://www.nist.gov/itl/iad/mig/lorehlt- evaluations

.. CURRENT CHALLENGES
Our statistical machine translation systems are trained using Moses (Koehn et al., ).
We build phrase-based systems using standard features that are commonly used in recent sys-
tem submissions to WMT (Williams et al., ; Ding et al., ). While we consider here
only phrase-based systems, we note that there are other statistical machine translation ap-
proaches such as hierarchical phrase-based models (Chiang, ) and syntax-based models
(Galley et al., , ) that have been shown to give superior performance for language
pairs such as Chinese–English and German–English.
We carry out our experiments on English–Spanish and German–English. For these lan-
guage pairs, large training data sets are available. We use datasets from the shared translation
task organized alongside the Conference on Machine Translation (WMT). For the domain
experiments, we use the OPUS corpus (Tiedemann, ).
Except for the domain experiments, we use the WMT test sets composed of news stories,
which are characterized by a broad range of topic, formal language, relatively long sentences
(about  words on average), and high standards for grammar, orthography, and style.
Domain Mismatch
A known challenge in translation is that in different domains, words have different trans-
lations and meaning is expressed in different styles. Hence, a crucial step in developing ma-
chine translation systems targeted at a speciﬁc use case is domain adaptation. We expect that
methods for domain adaptation will be developed for neural machine translation. A currently
popular approach is to train a general domain system, followed by training on in-domain data
for a few epochs (Luong and Manning, ; Freitag and Al-Onaizan, ).
Often, large amounts of training data are only available out of domain, but we still seek to
have robust performance. To test how well neural machine translation and statistical machine
translation hold up, we trained ﬁve different systems using different corpora obtained from
OPUS (Tiedemann, ). An additional system was trained on all the training data. Statistics
about corpus sizes are shown in Table .. Note that these domains are quite distant from each
other, much more so than, say, Europarl, TED Talks, News Commentary, and Global Voices.
We trained both statistical machine translation and neural machine translation systems for
all domains. All systems were trained for German-English, with tuning and test sets sub-
sampled from the data (these were not used in training). A common byte-pair encoding is
used for all training runs.
See Figure . for results. While the in-domain neural and statistical machine translation
systems are similar (neural machine translation is better for IT and Subtitles, statistical machine
translation is better for Law, Medical, and Koran), the out-of-domain performance for the neu-
ral machine translation systems is worse in almost all cases, sometimes dramatically so. For
http://www.stat.org/moses/
http://www.statmt.org/wmt/
http://opus.lingfil.uu.se/
We use the customary deﬁnition of domain in machine translation: a domain is deﬁned by a corpus from a
speciﬁc source, and may differ from other domains in topic, genre, style, level of formality, etc.

CHAPTER . NEURAL MACHINE TRANSLATION
Corpus
Words
Sentences
W/S
Law (Acquis)
Medical (EMEA)
IT
Koran (Tanzil)
Subtitles
Table .: Corpora used to train domain-speciﬁc systems, taken from the OPUS repository. IT corpora
are GNOME, KDE, PHP, Ubuntu, and OpenOfﬁce.
System ↓
Law
Medical
IT
Koran
Subtitles
All Data
Law
Medical
IT
Koran
Subtitles
Figure .: Quality of systems (BLEU), when trained on one domain (rows) and tested on another
domain (columns). Comparably, neural machine translation systems (left bars) show more degraded
performance out of domain.

.. CURRENT CHALLENGES
Source
Schaue um dich herum.
Reference
Look around you.
All
NMT: Look around you.
SMT: Look around you.
Law
NMT: Sughum gravecorn.
SMT: In order to implement dich Schaue .
Medical
NMT: EMEA / MB /  / -EN-Final Work progamme for
SMT: Schaue by dich around .
IT
NMT: Switches to paused.
SMT: To Schaue by itself . \t \t
Koran
NMT: Take heed of your own souls.
SMT: And you see.
Subtitles
NMT: Look around you.
SMT: Look around you .
Figure .: Examples for the translation of a sentence from the Subtitles corpus, when translated
with systems trained on different corpora. Performance out-of-domain is dramatically worse for neural
machine translation.
instance the Medical system leads to a BLEU score of . (neural machine translation) vs. .
(statistical machine translation) on the Law test set.
Figure . displays an example. When translating the sentence Schaue um dich herum. (ref-
erence: Look around you.) from the Subtitles corpus, we see mostly non-sensical and completely
unrelated output from the neural machine translation system. For instance, the translation
from the IT system is Switches to paused.
Note that the output of the neural machine translation system is often quite ﬂuent (e.g.,
Take heed of your own souls.) but completely unrelated to the input, while the statistical machine
translation output betrays its difﬁculties with coping with the out-of-domain input by leaving
some words untranslated (e.g., Schaue by dich around.). This is of particular concern when MT
is used for information gisting — the user will be mislead by hallucinated content in the neural
machine translation output.
Amount of Training Data
A well-known property of statistical systems is that increasing amounts of training data lead
to better results. In statistical machine translation systems, we have previously observed that
doubling the amount of training data gives a ﬁxed increase in BLEU scores. This holds true for
both parallel and monolingual data (Turchi et al., ; Irvine and Callison-Burch, ).
How do the data needs of statistical machine translation and neural machine translation
compare? Neural machine translation promises both to generalize better (exploiting word sim-
ilarity in embeddings) and condition on larger context (entire input and all prior output words).

CHAPTER . NEURAL MACHINE TRANSLATION
BLEU Scores with Varying Amounts of Training Data
Phrase-Based with Big LM
Phrase-Based
Neural
Corpus Size (English Words)
Figure .: BLEU scores for English-Spanish systems trained on . million to . million words of
parallel data. Quality for neural machine translation starts much lower, outperforms statistical machine
translation at about  million words, and even beats a statistical machine translation system with a big
billion word in-domain language model under high-resource conditions.

.. CURRENT CHALLENGES
Ratio
Words
Source: A Republican strategy to counter the re-election of Obama
. million
Un órgano de coordinación para el anuncio de libre determinación
. million
Lista de una estrategia para luchar contra la elección de hojas de Ohio
. million
Explosión realiza una estrategia divisiva de luchar contra las elecciones de autor
. million
Una estrategia republicana para la eliminación de la reelección de Obama
. million
Estrategia siria para contrarrestar la reelección del Obama .
. million
Una estrategia republicana para contrarrestar la reelección de Obama
Figure .: Translations of the ﬁrst sentence of the test set using neural machine translation system
trained on varying amounts of training data. Under low resource conditions, neural machine translation
produces ﬂuent output unrelated to the input.
We built English-Spanish systems on WMT data, about . million English words paired
with Spanish. To obtain a learning curve, we used
, ...,
, and all of the data. For statis-
tical machine translation, the language model was trained on the Spanish part of each subset,
respectively. In addition to a neural and statistical machine translation system trained on each
subset, we also used all additionally provided monolingual data for a big language model in
contrastive statistical machine translation systems.
Results are shown in Figure .. Neural machine translation exhibits a much steeper
learning curve, starting with abysmal results (BLEU score of . vs. . for
of the data),
outperforming statistical machine translation . vs. . with
of the data (. million
words), and even beating the statistical machine translation system with a big language model
with the full data set (. for neural machine translation, . for statistical machine transla-
tion, . for statistical with a big language model).
The contrast between the neural and statistical machine translation learning curves is quite
striking. While neural machine translation is able to exploit increasing amounts of training data
more effectively, it is unable to get off the ground with training corpus sizes of a few million
words or less.
To illustrate this, see Figure .. With
of the training data, the output is completely
unrelated to the input, some key words are properly translated with
and
of the data
(estrategia for strategy, elección or elecciones for election), and starting with
the translations
become respectable.
Noisy Data
Statistical machine translation is fairly robust to noisy data. The quality of systems holds up
fairly well, even if large parts of the training data are corrupted in various ways, such as mis-
aligned sentences, content in wrong languages, badly translated sentences, etc. Statistical ma-
chine translation models are built on probability distributions estimated from many occur-
rences of words and phrases. Any unsystematic noise in the training only affects the tail end of
the distribution.
Spanish was last represented in , we used data from http://statmt.org/wmt/translation-task.html

CHAPTER . NEURAL MACHINE TRANSLATION
Ratio shufﬂed
SMT (BLEU)
. (–.)
. (–.)
. (–.)
NMT (BLEU)
. (–.)
. (–.)
. (–.)
Table .: Impact of noise in the training data, with parts of the training corpus shufﬂed to contain
mis-aligned sentence pairs. Neural machine translation degrades severely, while statistical machine
translation holds up fairly well.
Is this still the case for neural machine translation? Chen et al. (a) considered one kind
of noise: misaligned sentence pairs in an experiments with a large English–French parallel
corpus. They shufﬂe the target side of part of the training corpus, so that these sentence pairs
are mis-aligned.
Table . shows the result. Statistical machine translation systems hold up fairly well.
Even with % of the data perturbed, the quality only drops from . to . BLEU points,
about what is to be expected with half the valid training data. However, the neural machine
translation system degrades severely, from . to . BLEU points, a drop of . points, com-
pared to the . point drop for statistical systems.
A possible explanation for this poor behavior of neural machine translation models is that
its prediction has to ﬁnd a good balance between language model and input context as the main
driver. When training observes increasing ratios of training example, for which the input sen-
tence is a meaningless distraction, it may generally learn to rely more on the output language
model aspect, hence hallucinating ﬂuent by inadequate output.
Word Alignment
The key contribution of the attention model in neural machine translation (Bahdanau et al.,
) was the imposition of an alignment of the output words to the input words. This takes
the shape of a probability distribution over the input words which is used to weigh them in a
bag-of-words representation of the input sentence.
Arguably, this attention model does not functionally play the role of a word alignment
between the source in the target, at least not in the same way as its analog in statistical machine
translation. While in both cases, alignment is a latent variable that is used to obtain probability
distributions over words or phrases, arguably the attention model has a broader role. For
instance, when translating a verb, attention may also be paid to its subject and object since these
may disambiguate it. To further complicate matters, the word representations are products of
bidirectional gated recurrent neural networks that have the effect that each word representation
is informed by the entire sentence context.
But there is a clear need for an alignment mechanism between source and target words.
For instance, prior work used the alignments provided by the attention model to interpolate
word translation decisions with traditional probabilistic dictionaries (Arthur et al., ), for
the introduction of coverage and fertility models (Tu et al., b), etc.

.. CURRENT CHALLENGES
Verhältnis
Obama
und
Netanyahu
Jahren
das
zwischen
ist
seit
gespannt
between
relations
and
Netanyahu
have
for
years
Obama
been
strained
the
relationship
die
Beziehungen
between
zwischen
Obama
Obama
and
Netanyahu
und
Netanjahu
has
been
sind
seit

stretched
Jahren
for
years
angespannt
(a) Desired Alignment
(b) Mismatched Alignment
Figure .: Word alignment for English–German: comparing the attention model states (green boxes
with probability in percent if over ) with alignments obtained from fast-align (blue outlines).
But is the attention model in fact the proper means? To examine this, we compare the
soft alignment matrix (the sequence of attention vectors) with word alignments obtained by
traditional word alignment methods. We use incremental fast-align (Dyer et al., ) to align
the input and output of the neural machine system.
See Figure .a for an illustration. We compare the word attention states (green boxes)
with the word alignments obtained with fast align (blue outlines). For most words, these match
up pretty well. Both attention states and fast-align alignment points are a bit fuzzy around the
function words have-been/sind.
However, the attention model may settle on alignments that do not correspond with our in-
tuition or alignment points obtained with fast-align. See Figure .b for the reverse language
direction, German–English. All the alignment points appear to be off by one position. We are
not aware of any intuitive explanation for this divergent behavior — the translation quality is
high for both systems.
We measure how well the soft alignment (attention model) of the neural machine transla-
tion system match the alignments of fast-align with two metrics:
a match score that checks for each output if the aligned input word according to fast-align
is indeed the input word that received the highest attention probability, and
a probability mass score that sums up the probability mass given to each alignment point
obtained from fast-align.

CHAPTER . NEURAL MACHINE TRANSLATION
Language Pair
Match
Prob.
German–English
English–German
Czech–English
English–Czech
Russian–English
English–Russian
Table .: Scores indicating overlap between attention probabilities and alignments obtained with fast-
align.
In these scores, we have to handle byte pair encoding and many-to-many alignments
In out experiment, we use the neural machine translation models provided by Edinburgh
(Sennrich et al., b). We run fast-align on the same parallel data sets to obtain alignment
models and used them to align the input and output of the neural machine translation system.
Table . shows alignment scores for the systems. The results suggest that, while drastic, the
divergence for German–English is an outlier. We note, however, that we have seen such large
a divergence also under different data conditions.
Note that the attention model may produce better word alignments by guided alignment
training (Chen et al., b; Liu et al., ) where supervised word alignments (such as the
ones produced by fast-align) are provided to model training.
Beam Search
The task of decoding is to ﬁnd the full sentence translation with the highest probability. In sta-
tistical machine translation, this problem has been addressed with heuristic search techniques
that explore a subset of the space of possible translation. A common feature of these search
techniques is a beam size parameter that limits the number of partial translations maintained
per input word.
There is typically a straightforward relationship between this beam size parameter and the
model score of resulting translations and also their quality score (e.g., BLEU). While there are
diminishing returns for increasing the beam parameter, typically improvements in these scores
can be expected with larger beams.
Decoding in neural translation models can be set up in similar fashion. When predicting
the next output word, we may not only commit to the highest scoring word prediction but
() neural machine translation operates on subwords, but fast-align is run on full words. () If an input word
is split into subwords by byte pair encoding, then we add their attention scores. () If an output word is split into
subwords, then we take the average of their attention vectors. () The match scores and probability mass scores are
computed as average over output word-level scores. () If an output word has no fast-align alignment point, it is
ignored in this computation. () If an output word is fast-aligned to multiple input words, then (a) for the match
score: count it as correct if the n aligned words among the top n highest scoring words according to attention and
(b) for the probability mass score: add up their attention scores.
https://github.com/rsennrich/wmt-scripts

CHAPTER . NEURAL MACHINE TRANSLATION
also maintain the next best scoring words in a list of partial translations. We record with each
partial translation the word translation probabilities (obtained from the softmax), extend each
partial translation with subsequent word predictions and accumulate these scores. Since the
number of partial translation explodes exponentially with each new output word, we prune
them down to a beam of highest scoring partial translations.
As in traditional statistical machine translation decoding, increasing the beam size allows
us to explore a larger set of the space of possible translation and hence ﬁnd translations with
better model scores.
However, as Figure . illustrates, increasing the beam size does not consistently improve
translation quality. In fact, in almost all cases, worse translations are found beyond an optimal
beam size setting (we are using again Edinburgh’s WMT  systems). The optimal beam size
varies from  (e.g., Czech–English) to around  (English–Romanian).
Normalizing sentence level model scores by length of the output alleviates the problem
somewhat and also leads to better optimal quality in most cases ( of the  language pairs
investigated). Optimal beam sizes are in the range of – in almost all cases, but quality still
drops with larger beams. The main cause of deteriorating quality are shorter translations under
wider beams.
Further Readings
Other studies have looked at the comparable performance of neural and statistical machine
translation systems. Bentivogli et al. () considered different linguistic categories for English–
German and Toral and Sánchez-Cartagena () compared different broad aspects such as
ﬂuency and reordering for nine language directions.
Additional Topics
Especially early work on neural networks for machine translation was aimed at building neural compo-
nents to be used in traditional statistical machine translation systems.
Translation Models
By including aligned source words in the conditioning context, Devlin et al.
() enrich a feed-forward neural network language model with source context Zhang et al. ()
add a sentence embedding to the conditional context of this model, which are learned using a variant
of convolutional neural networks and mapping them across languages. Meng et al. () use a more
complex convolutional neural network to encode the input sentence that uses gated layers and also
incorporates information about the output context.
Reordering Models
Lexicalized reordering models struggle with sparse data problems when con-
ditioned on rich context. Li et al. () show that a neural reordering model can be conditioned on
current and previous phrase pair (encoded with a recursive neural network auto-encoder) to make the
same classiﬁcation decisions for orientation type.

.. ADDITIONAL TOPICS
Pre-Ordering
Instead of handing reordering within the decoding process, we may pre-order the input
sentence into output word order. de Gispert et al. () use an input dependency tree to learn a model
that swaps children nodes and implement it using a feed-forward neural network. Miceli Barone and
Attardi () formulate a top-down left-to-right walk through the dependency tree and make reorder-
ing decisions at any node. They model this process with a recurrent neural network that includes past
decisions in the conditioning context.
N-Gram Translation Models
An alternative view of the phrase based translation model is to break up
phrase translations into minimal translation units, and employing a n-gram model over these units to
condition each minimal translation units on the previous ones. Schwenk et al. () treat each minimal
translation unit as an atomic symbol and train a neural language model over it. Alternatively, (Hu et al.,
) represent the minimal translation units as bag of words, (Wu et al., ) break them even further
into single input words, single output words, or single input-output word pairs, and Yu and Zhu ()
use phrase embeddings leaned with an auto-encoder.
